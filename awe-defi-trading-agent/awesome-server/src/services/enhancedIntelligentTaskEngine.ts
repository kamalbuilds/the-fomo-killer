import { ChatOpenAI } from '@langchain/openai';
import { HumanMessage, SystemMessage } from '@langchain/core/messages.js';
import { logger } from '../utils/logger.js';
import { MCPManager } from './mcpManager.js';
import { MCPToolAdapter } from './mcpToolAdapter.js';
import { MCPAuthService } from './mcpAuthService.js';
import { getTaskService } from './taskService.js';
import { taskExecutorDao } from '../dao/taskExecutorDao.js';
import { messageDao } from '../dao/messageDao.js';
import { conversationDao } from '../dao/conversationDao.js';
import { MessageType, MessageIntent, MessageStepType } from '../models/conversation.js';

/**
 * å·¥ä½œæµæ­¥éª¤ - åŸºäºTaskAnalysisServiceæ„å»ºçš„ç»“æ„
 */
export interface WorkflowStep {
  step: number;
  mcp: string;
  action: string;
  input?: any;
  // å¢å¼ºå­—æ®µ
  status?: 'pending' | 'executing' | 'completed' | 'failed';
  result?: any;
  error?: string;
  attempts?: number;
  maxRetries?: number;
}

/**
 * å¢å¼ºçš„å·¥ä½œæµçŠ¶æ€
 */
export interface EnhancedWorkflowState {
  taskId: string;
  originalQuery: string;
  workflow: WorkflowStep[];
  currentStepIndex: number;
  executionHistory: Array<{
    stepNumber: number;
    mcpName: string;
    action: string;
    success: boolean;
    result?: any;
    error?: string;
    timestamp: Date;
  }>;
  dataStore: Record<string, any>;
  isComplete: boolean;
  totalSteps: number;
  completedSteps: number;
  failedSteps: number;
}

/**
 * å¢å¼ºçš„æ™ºèƒ½Taskæ‰§è¡Œå¼•æ“
 * ä¸“æ³¨äºæ‰§è¡Œå·²æ„å»ºçš„MCPå·¥ä½œæµï¼Œç»“åˆAgentå¼•æ“çš„æ™ºèƒ½åŒ–ä¼˜åŠ¿
 */
export class EnhancedIntelligentTaskEngine {
  private llm: ChatOpenAI;
  private mcpManager: MCPManager;
  private mcpToolAdapter: MCPToolAdapter;
  private mcpAuthService: MCPAuthService;
  private taskService: any;

  constructor() {
    this.llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: 'gpt-4o',
      temperature: 0.1,
    });

    this.mcpManager = new MCPManager();
    this.mcpToolAdapter = new MCPToolAdapter(this.mcpManager);
    this.mcpAuthService = new MCPAuthService();
    this.taskService = getTaskService();
  }

  /**
   * å¢å¼ºçš„Taskæµå¼æ‰§è¡Œ - åŸºäºå·²æ„å»ºçš„å·¥ä½œæµ
   */
  async *executeWorkflowEnhanced(
    taskId: string,
    mcpWorkflow: any
  ): AsyncGenerator<{ event: string; data: any }, boolean, unknown> {
    
    // ğŸ”§ CRITICAL DEBUG: ç¡®è®¤è¿›å…¥Enhancedå¼•æ“
    logger.info(`ğŸš¨ ENHANCED ENGINE STARTED - Task: ${taskId}`);
    logger.info(`ğŸš€ Starting enhanced workflow execution [Task: ${taskId}]`);

    // ğŸ”§ éªŒè¯å·¥ä½œæµç»“æ„
    if (!mcpWorkflow || !mcpWorkflow.workflow || mcpWorkflow.workflow.length === 0) {
      yield {
        event: 'error',
        data: { 
          message: 'No valid workflow found. Please run task analysis first.',
          details: 'The task must be analyzed to generate a workflow before execution.'
        }
      };
      return false;
    }

    // ğŸ§  æ™ºèƒ½ä»»åŠ¡å¤æ‚åº¦åˆ†æ
    const task = await this.taskService.getTaskById(taskId);
    const taskQuery = task?.content || '';
    const taskComplexity = await this.analyzeTaskComplexity(taskQuery, mcpWorkflow.workflow.length);
    
    logger.info(`ğŸ¯ Task complexity analysis: ${taskComplexity.type} (${taskComplexity.recommendedObservation})`);

    // ğŸ”§ æ ¹æ®å¤æ‚åº¦è°ƒæ•´æ‰§è¡Œç­–ç•¥
    const shouldObserveEveryStep = taskComplexity.type !== 'simple_query';

    // ğŸ”§ å‘é€æ‰§è¡Œå¼€å§‹äº‹ä»¶ - å¯¹é½ä¼ ç»Ÿä»»åŠ¡æ‰§è¡Œäº‹ä»¶åç§°
    yield {
      event: 'execution_start',
      data: {
        taskId,
        agentName: 'WorkflowEngine',
        taskComplexity: taskComplexity.type,
        observationStrategy: taskComplexity.recommendedObservation,
        timestamp: new Date().toISOString(),
        message: `Starting execution...`,
        mode: 'enhanced',
        workflowInfo: {
          totalSteps: mcpWorkflow.workflow.length,
          mcps: mcpWorkflow.mcps?.map((mcp: any) => mcp.name) || []
        }
      }
    };

    // ğŸ”§ åˆå§‹åŒ–å¢å¼ºçš„å·¥ä½œæµçŠ¶æ€
    const state: EnhancedWorkflowState = {
      taskId,
      originalQuery: '', // ä»taskè·å–
      workflow: mcpWorkflow.workflow.map((step: any, index: number) => ({
        ...step,
        status: 'pending',
        attempts: 0,
        maxRetries: 2
      })),
      currentStepIndex: 0,
      executionHistory: [],
      dataStore: {},
      isComplete: false,
      totalSteps: mcpWorkflow.workflow.length,
      completedSteps: 0,
      failedSteps: 0
    };

    try {
      // ğŸ”§ è·å–ä»»åŠ¡ä¿¡æ¯
      const task = await this.taskService.getTaskById(taskId);
      if (task) {
        state.originalQuery = task.content;
      }

      // ğŸ”§ å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
      await this.prepareWorkflowExecution(taskId, state, mcpWorkflow);

      // ğŸ”§ ç§»é™¤workflow_execution_startäº‹ä»¶ï¼Œç›´æ¥å¼€å§‹æ­¥éª¤æ‰§è¡Œ

      for (let i = 0; i < state.workflow.length; i++) {
        const currentStep = state.workflow[i];
        state.currentStepIndex = i;

        logger.info(`ğŸ§  Executing workflow step ${currentStep.step}: ${currentStep.mcp}.${currentStep.action}`);

        // ğŸ”§ é¢„å¤„ç†å‚æ•°ï¼šæ™ºèƒ½å‚æ•°å¤„ç†ï¼Œä½¿ç”¨LLMåˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ¨æ–­
        let processedInput = currentStep.input || {};
        
        // ğŸ§  ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ¨æ–­è¾“å…¥
        const shouldReinferInput = await this.shouldReinferStepInput(currentStep, state, processedInput);
        
        if (!currentStep.input || shouldReinferInput || state.dataStore.lastResult) {
          logger.info(`ğŸ”„ Inferring step input from context (LLMåˆ¤æ–­éœ€è¦é‡æ–°æ¨æ–­: ${shouldReinferInput})`);
          processedInput = await this.inferStepInputFromContext(currentStep, state);
        }

        // ğŸ”§ ç¡®å®šå·¥å…·ç±»å‹å’Œæ™ºèƒ½æè¿° - ä¸Agentå¼•æ“ä¸€è‡´
        const isLLMTool = currentStep.mcp === 'llm' || currentStep.mcp.toLowerCase().includes('llm');
        const toolType = isLLMTool ? 'llm' : 'mcp';
        const mcpName = isLLMTool ? null : currentStep.mcp;
        
        // ğŸ”§ é¢„å…ˆæ¨æ–­å®é™…å·¥å…·åç§°
        let actualToolName = currentStep.action;
        if (!isLLMTool) {
          const task = await this.taskService.getTaskById(state.taskId);
          if (task) {
            logger.info(`ğŸ” Inferring tool name for step ${currentStep.step}: ${currentStep.mcp}.${currentStep.action}`);
            actualToolName = await this.inferActualToolName(currentStep.mcp, currentStep.action, processedInput, task.userId);
            logger.info(`âœ… Tool name inference completed: ${actualToolName}`);
          }
        }

        // ğŸ”§ ç”Ÿæˆç®€å•çš„expectedOutputå’Œreasoningï¼ˆä½¿ç”¨å®é™…å·¥å…·åç§°ï¼‰
        const expectedOutput = isLLMTool 
          ? `AI analysis and processing for ${actualToolName}`
          : `Execute ${actualToolName} on ${currentStep.mcp}`;
        const reasoning = `Workflow step ${currentStep.step}`;

        // ğŸ”§ å‘é€æ­¥éª¤å¼€å§‹äº‹ä»¶ - å¯¹é½ä¼ ç»Ÿä»»åŠ¡æ‰§è¡Œäº‹ä»¶åç§°
        const stepId = `workflow_step_${currentStep.step}_${Date.now()}`;
        yield {
          event: 'step_executing',
          data: {
            step: currentStep.step,
            mcpName: mcpName || currentStep.mcp,
            actionName: actualToolName,
            input: JSON.stringify(processedInput),
            agentName: 'WorkflowEngine',
            message: `WorkflowEngine is executing step ${currentStep.step}: ${actualToolName}`,
            // ğŸ”§ ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´çš„toolDetailsç»“æ„
            toolDetails: {
              toolType: toolType,
              toolName: actualToolName,
              mcpName: mcpName,
              args: processedInput,
              expectedOutput: expectedOutput,
              reasoning: reasoning,
              timestamp: new Date().toISOString()
            }
          }
        };

        // ğŸ”§ æ‰§è¡Œå½“å‰æ­¥éª¤ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰- ä¼ é€’é¢„å¤„ç†çš„å‚æ•°å’Œå®é™…å·¥å…·åç§°
        logger.info(`ğŸ”„ Starting execution for step ${currentStep.step} with tool: ${actualToolName}`);
        const executionResult = await this.executeWorkflowStepWithRetry(currentStep, state, processedInput, actualToolName);
        logger.info(`ğŸ“‹ Execution result:`, {
          success: executionResult.success,
          hasResult: !!executionResult.result,
          resultSize: executionResult.result ? JSON.stringify(executionResult.result).length : 0,
          error: executionResult.error || 'none'
        });

        // ğŸ”§ CRITICAL: æ£€æŸ¥æ˜¯å¦åˆ°è¾¾äº†åç»­å¤„ç†é˜¶æ®µ
        logger.info(`ğŸ¯ REACHED POST-EXECUTION PROCESSING - Step ${currentStep.step}`);

        // ğŸ”§ è®°å½•æ‰§è¡Œå†å²
        const historyEntry = {
          stepNumber: currentStep.step,
          mcpName: currentStep.mcp,
          action: currentStep.action,
          success: executionResult.success,
          result: executionResult.result,
          error: executionResult.error,
          timestamp: new Date()
        };
        state.executionHistory.push(historyEntry);

        // ğŸ”§ é‡è¦è°ƒè¯•ï¼šæ£€æŸ¥executionResultçš„ç»“æ„
        logger.info(`ğŸ” CRITICAL DEBUG - executionResult:`, {
          success: executionResult.success,
          hasResult: !!executionResult.result,
          resultType: typeof executionResult.result,
          resultKeys: executionResult.result ? Object.keys(executionResult.result) : 'no result'
        });

        // ğŸ”§ å‘é€step_raw_resultäº‹ä»¶ï¼ˆæ–°å¢äº‹ä»¶ï¼‰
        if (executionResult.success && executionResult.result) {
          logger.info(`ğŸ¯ CRITICAL DEBUG - Conditions met, yielding step_raw_result`);
          
          yield {
            event: 'step_raw_result',
            data: {
              step: currentStep.step,
              success: true,
              result: executionResult.result,  // åŸå§‹MCPæ•°æ®ç»“æ„
              agentName: 'WorkflowEngine',
              executionDetails: {
                toolType: toolType,
                toolName: actualToolName,
                mcpName: mcpName,
                // ğŸ”§ ç§»é™¤rawResulté‡å¤ - æ•°æ®å·²åœ¨ä¸Šé¢çš„resultå­—æ®µä¸­
                args: executionResult.actualArgs || currentStep.input || {},
                expectedOutput: expectedOutput,
                timestamp: new Date().toISOString()
              }
            }
          };

          // ğŸ”§ å¼‚æ­¥ä¿å­˜åŸå§‹ç»“æœï¼Œé¿å…é˜»å¡æµå¼å“åº”
          this.saveStepRawResult(taskId, currentStep.step, currentStep, executionResult.result, executionResult.actualArgs, toolType, mcpName, expectedOutput, reasoning, actualToolName).catch(error => {
            logger.error(`Failed to save step raw result:`, error);
          });
        }

        // ğŸ”§ æµå¼æ ¼å¼åŒ–ç»“æœå¤„ç†ï¼ˆå‚è€ƒAgentå¼•æ“ï¼‰
        let formattedResult = '';
        if (executionResult.success && executionResult.result) {
          // ğŸ”§ æµå¼æ ¼å¼åŒ–ï¼šå…ˆå‘é€æµå¼æ ¼å¼åŒ–å—ï¼ˆä»…å¯¹MCPå·¥å…·ï¼‰
          if (toolType === 'mcp') {
            const formatGenerator = this.formatAndStreamTaskResult(
              executionResult.result,
              currentStep.mcp,
              actualToolName
            );

            // ğŸ”§ ä½¿ç”¨å‰ç«¯å¯¹åº”çš„äº‹ä»¶åç§°
            if (currentStep.step === state.totalSteps) {
              // æœ€åä¸€æ­¥ï¼šå‘é€step_startäº‹ä»¶ç„¶åä½¿ç”¨summary_chunkäº‹ä»¶
              yield {
                event: 'step_start',
                data: {
                  message: `Running ${mcpName || ''} - ${actualToolName || ''}`,
                  agentName: 'WorkflowEngine'
                }
              };
              
              for await (const chunk of formatGenerator) {
                yield {
                  event: 'summary_chunk',
                  data: {
                    content: chunk,
                    agentName: 'WorkflowEngine'
                  }
                };
              }
            } else {
              // ä¸­é—´æ­¥éª¤ï¼šæš‚æ—¶è·³è¿‡æµå¼è¾“å‡ºï¼Œåªä¿ç•™æœ€ç»ˆæ ¼å¼åŒ–ç»“æœ
            }
          }

          // ğŸ”§ ç”Ÿæˆå®Œæ•´çš„æ ¼å¼åŒ–ç»“æœç”¨äºå­˜å‚¨å’Œæœ€ç»ˆäº‹ä»¶
          formattedResult = await this.generateFormattedResult(
            executionResult.result,
            currentStep.mcp,
            actualToolName
          );

          // ğŸ”§ ç§»é™¤step_formatted_resultäº‹ä»¶ï¼Œå‰ç«¯ä¸éœ€è¦

          // ğŸ”§ å¼‚æ­¥ä¿å­˜æ ¼å¼åŒ–ç»“æœï¼Œé¿å…é˜»å¡æµå¼å“åº”
          this.saveStepFormattedResult(taskId, currentStep.step, currentStep, formattedResult, executionResult.actualArgs, toolType, mcpName, expectedOutput, reasoning, actualToolName).catch(error => {
            logger.error(`Failed to save step formatted result:`, error);
          });

          // ğŸ”§ æ›´æ–°æ•°æ®å­˜å‚¨
          state.dataStore[`step_${currentStep.step}_result`] = executionResult.result;
          state.dataStore.lastResult = executionResult.result;
        }

        // ğŸ”§ æ›´æ–°æ­¥éª¤çŠ¶æ€
        if (executionResult.success) {
          currentStep.status = 'completed';
          state.completedSteps++;
          
          // ğŸ”§ å‘é€step_completeäº‹ä»¶ - å¯¹é½ä¼ ç»Ÿä»»åŠ¡æ‰§è¡Œæ ¼å¼
          yield {
            event: 'step_complete',
            data: {
              step: currentStep.step,
              success: true,
              result: formattedResult || executionResult.result, // æ ¼å¼åŒ–ç»“æœä¾›å‰ç«¯æ˜¾ç¤º
              rawResult: executionResult.result, // ä¿ç•™åŸå§‹MCPç»“æœä¾›è°ƒè¯•
              // ğŸ”§ ä¿ç•™æ™ºèƒ½å¼•æ“çš„å¢å¼ºå­—æ®µ
              agentName: 'WorkflowEngine',
              message: `WorkflowEngine completed step ${currentStep.step} successfully`,
              progress: {
                completed: state.completedSteps,
                total: state.totalSteps,
                percentage: Math.round((state.completedSteps / state.totalSteps) * 100)
              }
            }
          };
        } else {
          currentStep.status = 'failed';
          state.failedSteps++;

          // ğŸ”§ å‘é€step_erroräº‹ä»¶ - ç®€åŒ–æ ¼å¼
          yield {
            event: 'step_error',
            data: {
              step: currentStep.step,
              error: executionResult.error,
              agentName: 'WorkflowEngine'
            }
          };
        }

        // ğŸ¯ ä¸Agentå¼•æ“ä¿æŒä¸€è‡´ï¼šä½¿ç”¨æ™ºèƒ½è§‚å¯Ÿé˜¶æ®µåˆ¤æ–­å®Œæˆ
        let shouldContinue = true;

        // ğŸ”§ æ‰§è¡ŒæˆåŠŸåè¿›è¡Œæ™ºèƒ½è§‚å¯Ÿåˆ¤æ–­ï¼ˆä¸Agentå¼•æ“ä¸€è‡´ï¼‰
        if (executionResult.success) {
          logger.info(`ğŸ” Task performing intelligent observation after step ${i + 1}`);
          
          const observationResult = await this.taskObservationPhase(state, taskComplexity);
          
          if (!observationResult.shouldContinue) {
            logger.info(`ğŸ¯ Task determined complete after intelligent observation`);
            shouldContinue = false;
            
            // å‘é€ä»»åŠ¡å®Œæˆäº‹ä»¶
            yield {
              event: 'task_observation_complete',
              data: {
                step: i + 1,
                message: 'Task determined complete by intelligent observation',
                reasoning: observationResult.newObjective || 'Task requirements fulfilled',
                taskComplete: true
              }
            };
          } else if (observationResult.newObjective) {
            logger.info(`ğŸ¯ Task next objective: ${observationResult.newObjective}`);
          }
        }

        // ğŸ”§ ç§»é™¤task_observationäº‹ä»¶ï¼Œå‰ç«¯ä¸éœ€è¦
        
        // ğŸ”„ ç®€åŒ–åŠ¨æ€è§„åˆ’é€»è¾‘ï¼ˆä¿ç•™å·¥ä½œæµé€‚åº”èƒ½åŠ›ä½†å‡å°‘å¤æ‚åº¦ï¼‰
        let shouldAdaptWorkflow = false;
        
        // åªåœ¨å¤±è´¥æ—¶è€ƒè™‘å·¥ä½œæµé€‚åº”
        if (!executionResult.success && i < state.workflow.length - 2) {
          shouldAdaptWorkflow = await this.shouldAdaptWorkflow(state, currentStep);
        }
        
        if (shouldAdaptWorkflow) {
          logger.info(`ğŸ§  Initiating dynamic workflow adaptation...`);
          
          const currentContext = this.buildCurrentContext(state);
          const planningResult = await this.taskDynamicPlanningPhase(state, currentContext);
          
          if (planningResult.success && planningResult.adaptedSteps) {
            // ç”¨åŠ¨æ€è§„åˆ’çš„æ­¥éª¤æ›¿æ¢å‰©ä½™å·¥ä½œæµ
            const adaptedWorkflow = planningResult.adaptedSteps.map((adaptedStep, index) => ({
              ...adaptedStep,
              step: i + index + 1,
              status: 'pending' as const,
              attempts: 0,
              maxRetries: 2
            }));
            
            // æ›´æ–°å·¥ä½œæµï¼šä¿ç•™å·²å®Œæˆçš„æ­¥éª¤ï¼Œæ›¿æ¢å‰©ä½™æ­¥éª¤
            state.workflow = [
              ...state.workflow.slice(0, i + 1),
              ...adaptedWorkflow
            ];
            state.totalSteps = state.workflow.length;
            
            // ğŸ”§ ç§»é™¤workflow_adaptedäº‹ä»¶ï¼Œå‰ç«¯ä¸éœ€è¦
            
            logger.info(`âœ… Workflow adapted: ${adaptedWorkflow.length} new steps planned`);
          }
        }
        
        // ğŸ¯ ç›´æ¥å®Œæˆæ£€æµ‹ï¼šå¦‚æœåˆ¤æ–­ä»»åŠ¡å·²å®Œæˆï¼Œç«‹å³é€€å‡º
        if (!shouldContinue) {
          logger.info(`ğŸ Task completion detected, stopping workflow execution`);
          break;
        }
      }

      // ğŸ”§ ç®€åŒ–é€»è¾‘ï¼šæ‰§è¡Œåˆ°æœ€åå°±æ˜¯æˆåŠŸ
      state.isComplete = true;

      // ğŸ”§ ç”Ÿæˆæœ€ç»ˆç»“æœ
      const finalResult = this.generateWorkflowFinalResult(state);
      const overallSuccess = true;
      
      // ğŸ”§ å‘é€generating_summaryäº‹ä»¶
      yield {
        event: 'generating_summary',
        data: {
          message: 'Generating summary...',
          agentName: 'WorkflowEngine'
        }
      };

      // ğŸ”§ å‘é€workflow_completeäº‹ä»¶
      yield {
        event: 'workflow_complete',
        data: {
          message: 'Workflow completed',
          agentName: 'WorkflowEngine'
        }
      };

      // ğŸ”§ å‘é€task_completeäº‹ä»¶
      yield {
        event: 'task_complete',
        data: {
          agentName: 'WorkflowEngine'
        }
      };

      // ğŸ”§ å‘é€final_resultäº‹ä»¶ï¼ˆç”¨äºä¸Šå±‚åˆ¤æ–­æˆåŠŸçŠ¶æ€ï¼‰
      yield {
        event: 'final_result',
        data: {
          success: overallSuccess,
          result: finalResult,
          agentName: 'WorkflowEngine'
        }
      };

      // ğŸ”§ ä¿å­˜æœ€ç»ˆç»“æœ
      await this.saveWorkflowFinalResult(taskId, state, finalResult);

      return overallSuccess;

    } catch (error) {
      logger.error(`âŒ Enhanced workflow execution failed:`, error);
      
      yield {
        event: 'error',
        data: {
          message: 'Enhanced workflow execution failed',
          details: error instanceof Error ? error.message : String(error)
        }
      };

      return false;
    }
  }

  /**
   * å‡†å¤‡å·¥ä½œæµæ‰§è¡Œç¯å¢ƒ
   */
  private async prepareWorkflowExecution(taskId: string, state: EnhancedWorkflowState, mcpWorkflow: any): Promise<void> {
    try {
      const task = await this.taskService.getTaskById(taskId);
      if (!task) {
        throw new Error('Task not found');
      }

      // ğŸ”§ ç¡®ä¿å·¥ä½œæµä¸­ç”¨åˆ°çš„MCPå·²è¿æ¥
      const requiredMCPs = [...new Set(state.workflow.map(step => step.mcp))];
      if (requiredMCPs.length > 0) {
        logger.info(`ğŸ“¡ Ensuring required MCPs are connected: ${requiredMCPs.join(', ')}`);
        await this.ensureWorkflowMCPsConnected(task.userId, taskId, requiredMCPs);
      }
    } catch (error) {
      logger.error('Failed to prepare workflow execution:', error);
      throw error;
    }
  }

  /**
   * ç¡®ä¿å·¥ä½œæµçš„MCPå·²è¿æ¥ - åŒæ­¥Agentå¼•æ“çš„å®Œæ•´æƒé™æ ¡éªŒé€»è¾‘
   */
  private async ensureWorkflowMCPsConnected(userId: string, taskId: string, mcpNames: string[]): Promise<void> {
    try {
      logger.info(`Ensuring MCP connections for workflow execution (User: ${userId}), required MCPs: ${mcpNames.join(', ')}`);

      // ğŸ”§ è·å–ä»»åŠ¡ä¿¡æ¯ä»¥è·å–å·¥ä½œæµMCPé…ç½®
      const task = await this.taskService.getTaskById(taskId);
      const mcpWorkflow = typeof task.mcpWorkflow === 'string' 
        ? JSON.parse(task.mcpWorkflow) 
        : task.mcpWorkflow;

      for (const mcpName of mcpNames) {
        try {
          logger.info(`ğŸ”— Ensuring MCP ${mcpName} is connected for workflow execution`);
          
          // æ£€æŸ¥MCPæ˜¯å¦å·²è¿æ¥
          const connectedMCPs = this.mcpManager.getConnectedMCPs(userId);
          const isConnected = connectedMCPs.some(mcp => mcp.name === mcpName);
          
          if (!isConnected) {
            logger.info(`MCP ${mcpName} not connected for user ${userId}, attempting to connect for workflow task...`);
            
            // ğŸ”§ è·å–MCPé…ç½®
            const { getPredefinedMCP } = await import('./predefinedMCPs.js');
            const mcpConfig = getPredefinedMCP(mcpName);
            
            if (!mcpConfig) {
              throw new Error(`MCP ${mcpName} configuration not found`);
            }

            // ğŸ”§ ä»å·¥ä½œæµä¸­æŸ¥æ‰¾MCPä¿¡æ¯ï¼ˆåŒæ­¥Agentå¼•æ“é€»è¾‘ï¼‰
            const mcpInfo = mcpWorkflow?.mcps?.find((mcp: any) => mcp.name === mcpName) || { name: mcpName, authRequired: mcpConfig.authRequired };

            let authenticatedMcpConfig = mcpConfig;

            // ğŸ”§ ä½¿ç”¨å·¥ä½œæµä¸­çš„authRequiredæ ‡è¯† - åŒæ­¥Agentå¼•æ“
            if (mcpInfo.authRequired) {
              // è·å–ç”¨æˆ·è®¤è¯ä¿¡æ¯
              const userAuth = await this.mcpAuthService.getUserMCPAuth(userId, mcpName);
              if (!userAuth || !userAuth.isVerified || !userAuth.authData) {
                throw new Error(`User authentication not found or not verified for MCP ${mcpName}. Please authenticate this MCP service first.`);
              }

              // åŠ¨æ€æ³¨å…¥è®¤è¯ä¿¡æ¯
              console.log(`\nğŸ”§ === MCP Auth Injection Debug (Enhanced Engine) ===`);
              console.log(`MCP Name: ${mcpName}`);
              console.log(`User ID: ${userId}`);
              console.log(`Task ID: ${taskId}`);
              console.log(`Auth Data Keys: ${Object.keys(userAuth.authData)}`);
              console.log(`Auth Params: ${JSON.stringify(mcpConfig.authParams, null, 2)}`);
              console.log(`Env Config: ${JSON.stringify(mcpConfig.env, null, 2)}`);
              
              const dynamicEnv = { ...mcpConfig.env };
              if (mcpConfig.env) {
                for (const [envKey, envValue] of Object.entries(mcpConfig.env)) {
                  console.log(`Checking env var: ${envKey} = "${envValue}"`);
                  
                  // ğŸ”§ æ”¹è¿›ï¼šæ£€æŸ¥ç”¨æˆ·è®¤è¯æ•°æ®ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„é”®
                  let authValue = userAuth.authData[envKey];
                  
                  // ğŸ”§ å¦‚æœç›´æ¥é”®åä¸å­˜åœ¨ï¼Œå°è¯•ä»authParamsæ˜ å°„ä¸­æŸ¥æ‰¾
                  if (!authValue && mcpConfig.authParams && mcpConfig.authParams[envKey]) {
                    const authParamKey = mcpConfig.authParams[envKey];
                    authValue = userAuth.authData[authParamKey];
                    console.log(`ğŸ”§ Trying authParams mapping: ${envKey} -> ${authParamKey}, value: "${authValue}"`);
                  }
                  
                  if ((!envValue || envValue === '') && authValue) {
                    dynamicEnv[envKey] = authValue;
                    console.log(`âœ… Injected ${envKey} = "${authValue}"`);
                    logger.info(`Injected authentication for ${envKey} in MCP ${mcpName} for user ${userId}`);
                  } else {
                    console.log(`âŒ Not injecting ${envKey}: envValue="${envValue}", authValue: "${authValue}"`);
                  }
                }
              }

              // åˆ›å»ºå¸¦è®¤è¯ä¿¡æ¯çš„MCPé…ç½®
              authenticatedMcpConfig = {
                ...mcpConfig,
                env: dynamicEnv
              };
            } else {
              logger.info(`MCP ${mcpName} does not require authentication, using default configuration`);
            }

            // ğŸ”§ ä½¿ç”¨connectPredefinedæ–¹æ³•å®ç°å¤šç”¨æˆ·éš”ç¦»
            const connected = await this.mcpManager.connectPredefined(authenticatedMcpConfig, userId);
            if (!connected) {
              throw new Error(`Failed to connect to MCP ${mcpName} for user ${userId}`);
            }

            logger.info(`âœ… Successfully connected MCP ${mcpName} for user ${userId} and workflow task`);
          } else {
            logger.info(`âœ… MCP ${mcpName} already connected for user ${userId}`);
          }
        } catch (error) {
          logger.error(`Failed to ensure MCP connection for ${mcpName} (User: ${userId}):`, error);
          throw new Error(`Failed to connect required MCP service ${mcpName}: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
      }

      logger.info(`âœ… All required MCP services connected for workflow execution (User: ${userId})`);
    } catch (error) {
      logger.error('Failed to ensure workflow MCPs connected:', error);
      throw error;
    }
  }

  /**
   * å¸¦é‡è¯•æœºåˆ¶çš„æ­¥éª¤æ‰§è¡Œ
   */
  private async executeWorkflowStepWithRetry(step: WorkflowStep, state: EnhancedWorkflowState, input: any, actualToolName?: string): Promise<{
    success: boolean;
    result?: any;
    error?: string;
    actualArgs?: any;
  }> {
    let lastError = '';
    const toolName = actualToolName || step.action;
    
    for (let attempt = 1; attempt <= (step.maxRetries || 2) + 1; attempt++) {
      step.attempts = attempt;
      
      try {
        logger.info(`ğŸ”§ Executing ${step.mcp}.${toolName} (attempt ${attempt})`);
        
        const result = await this.executeWorkflowStep(step, state, input, actualToolName);
        
        if (result.success) {
          logger.info(`âœ… Step ${step.step} execution successful on attempt ${attempt}`);
          return result;
        } else {
          lastError = result.error || 'Unknown error';
          logger.warn(`âš ï¸ Step ${step.step} failed on attempt ${attempt}: ${lastError}`);
          
          if (attempt <= (step.maxRetries || 2)) {
            logger.info(`ğŸ”„ Retrying step ${step.step} (${attempt}/${step.maxRetries || 2})`);
            await new Promise(resolve => setTimeout(resolve, 1000 * attempt)); // é€’å¢å»¶è¿Ÿ
          }
        }
      } catch (error) {
        lastError = error instanceof Error ? error.message : String(error);
        logger.error(`âŒ Step ${step.step} execution error on attempt ${attempt}:`, error);
        
        if (attempt <= (step.maxRetries || 2)) {
          await new Promise(resolve => setTimeout(resolve, 1000 * attempt));
        }
      }
    }
    
    return { success: false, error: lastError };
  }

  /**
   * æ‰§è¡Œå•ä¸ªå·¥ä½œæµæ­¥éª¤
   */
  private async executeWorkflowStep(step: WorkflowStep, state: EnhancedWorkflowState, input: any, actualToolName?: string): Promise<{
    success: boolean;
    result?: any;
    error?: string;
    actualArgs?: any;
  }> {
    try {
      const task = await this.taskService.getTaskById(state.taskId);
      if (!task) {
        throw new Error('Task not found');
      }

      // ğŸ”§ æ”¯æŒLLMå·¥å…·å’ŒMCPå·¥å…·
      const isLLMTool = step.mcp === 'llm' || step.mcp.toLowerCase().includes('llm');
      
      if (isLLMTool) {
        // LLMå·¥å…·æ‰§è¡Œ
        const toolName = actualToolName || step.action;
        logger.info(`ğŸ¤– Calling LLM with action: ${toolName}`);
        logger.info(`ğŸ“ Input: ${JSON.stringify(input, null, 2)}`);
        
        const prompt = `Execute ${toolName} with the following input: ${JSON.stringify(input, null, 2)}`;
        const response = await this.llm.invoke([new SystemMessage(prompt)]);
        const result = response.content as string;
        
        logger.info(`âœ… LLM ${toolName} execution successful`);
        return { success: true, result, actualArgs: input };
      } else {
        // MCPå·¥å…·æ‰§è¡Œ - ä½¿ç”¨é¢„æ¨æ–­çš„å®é™…å·¥å…·åç§°
        let toolName = actualToolName;
        if (!toolName) {
          try {
            toolName = await this.inferActualToolName(step.mcp, step.action, input, task.userId);
          } catch (error) {
            const errorMessage = error instanceof Error ? error.message : String(error);
            logger.error(`âŒ Failed to infer tool name for MCP ${step.mcp} action ${step.action}: ${errorMessage}`);
            throw error;
          }
        }
        
        logger.info(`ğŸ“¡ Calling MCP ${step.mcp} with action: ${step.action} (resolved to: ${toolName})`);
        logger.info(`ğŸ“ Input: ${JSON.stringify(input, null, 2)}`);

        // ğŸ”§ æ–°å¢ï¼šæ™ºèƒ½å‚æ•°è½¬æ¢ï¼Œç¡®ä¿å‚æ•°åä¸å·¥å…· schema åŒ¹é…
        let convertedInput = await this.convertParametersForMCP(step.mcp, toolName, input, task.userId);
        
        // ğŸ”§ NEW: ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å†…å®¹è½¬æ¢ï¼Œé˜²æ­¢å ä½ç¬¦é—®é¢˜
        const llmResult = await this.convertParametersWithLLM(toolName, convertedInput, step.mcp, task.userId);
        const smartToolName = llmResult.toolName;
        const smartInput = llmResult.params;
        logger.info(`ğŸ“ LLM Smart Tool Selection: ${toolName} â†’ ${smartToolName}`);
        logger.info(`ğŸ“ Final Converted Input: ${JSON.stringify(smartInput, null, 2)}`);

        // ğŸ”§ NEW: Twitterå·¥ä½œæµæ™ºèƒ½ä¿®å¤ - æ£€æµ‹å¹¶ä¿®å¤publish_draftæ— draft_idçš„é—®é¢˜
        const { finalToolName, finalInput } = await this.handleTwitterWorkflowFix(
          step.mcp, 
          smartToolName, 
          smartInput, 
          state, 
          task.userId
        );

        const result = await this.mcpToolAdapter.callTool(
          step.mcp,
          finalToolName,
          finalInput,
          task.userId
        );

        // ğŸ”§ æ–°å¢ï¼šx-mcpè‡ªåŠ¨å‘å¸ƒå¤„ç†ï¼ˆä¸Agentå¼•æ“ä¿æŒä¸€è‡´ï¼‰
        const finalResult = await this.handleXMcpAutoPublish(step.mcp, finalToolName, result, task.userId);
        
        logger.info(`âœ… MCP ${step.mcp} execution successful - returning original MCP structure`);
        return { success: true, result: finalResult, actualArgs: input };
      }

    } catch (error) {
      logger.error(`âŒ Workflow step execution failed:`, error);
      return {
        success: false,
        error: error instanceof Error ? error.message : String(error)
      };
    }
  }

  /**
   * ğŸ§  ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ¨æ–­æ­¥éª¤è¾“å…¥
   */
  private async shouldReinferStepInput(step: WorkflowStep, state: EnhancedWorkflowState, currentInput: any): Promise<boolean> {
    try {
      const judgmentPrompt = `You are an expert data quality analyst. Your task is to determine if step input data contains placeholders, incomplete information, or needs to be re-inferred from previous results.

**Current Step:**
- Action: ${step.action}
- MCP: ${step.mcp}
- Current Input: ${JSON.stringify(currentInput, null, 2)}

**Previous Execution Results:**
${state.executionHistory.length > 0 ? state.executionHistory.map((entry, index) => `
Step ${entry.stepNumber}: ${entry.action}
- Success: ${entry.success}
- Result Available: ${entry.result ? 'Yes' : 'No'}
${entry.result ? `- Result Preview: ${JSON.stringify(entry.result, null, 2).substring(0, 500)}...` : ''}
`).join('\n') : 'No previous results available'}

**Data Store:**
${JSON.stringify(state.dataStore, null, 2)}

**INTELLIGENT ANALYSIS:**

Analyze the current input and determine if it needs to be re-inferred. Consider:

1. **Placeholder Detection**: Does the input contain obvious placeholders, template text, or incomplete data?
2. **Relevance Check**: Is the current input appropriate for the intended action?
3. **Data Dependency**: Should this step use data from previous steps instead of the current input?
4. **Completeness**: Is the input sufficient and meaningful for the tool to execute successfully?

**Examples of inputs that NEED re-inference:**
- Contains "[Insert", "placeholder", "template", "example"
- Generic descriptive text instead of actual data
- Empty or null required parameters
- Mismatched data types for the action
- References to unavailable data

**Examples of inputs that are GOOD:**
- Actual data values that match the tool requirements
- Properly formatted parameters
- Real content ready for processing
- Complete and meaningful data

**CRITICAL THINKING:**
- If previous steps have produced useful results, should this step use that data instead?
- Does the current input look like it was generated by a template rather than extracted from real data?
- Would a reasonable person consider this input ready for tool execution?

**OUTPUT FORMAT (JSON only):**
{
  "shouldReinfer": true/false,
  "reasoning": "Detailed explanation of why re-inference is or isn't needed",
  "confidence": 0.0-1.0
}

Analyze the data quality now:`;

      const response = await this.llm.invoke([new SystemMessage(judgmentPrompt)]);
      const responseText = response.content.toString().trim();
      
      try {
        let cleanedJson = responseText;
        cleanedJson = cleanedJson.replace(/```json\s*|\s*```/g, '');
        
        const jsonMatch = cleanedJson.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          const judgment = JSON.parse(jsonMatch[0]);
          
          logger.info(`ğŸ§  LLM Input Quality Judgment:`, {
            shouldReinfer: judgment.shouldReinfer,
            reasoning: judgment.reasoning,
            confidence: judgment.confidence
          });
          
          return judgment.shouldReinfer === true;
        }
      } catch (parseError) {
        logger.warn(`âš ï¸ Failed to parse LLM judgment, defaulting to safe re-inference: ${parseError}`);
      }
      
      // é™çº§æ–¹æ¡ˆï¼šå¦‚æœè§£æå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å‰ä¸€æ­¥çš„ç»“æœå¯ç”¨
      return state.dataStore.lastResult !== undefined;
      
    } catch (error) {
      logger.error(`âŒ Input quality judgment failed: ${error}`);
      // é™çº§æ–¹æ¡ˆï¼šå¦‚æœåˆ¤æ–­å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å‰ä¸€æ­¥çš„ç»“æœ
      return state.dataStore.lastResult !== undefined;
    }
  }

  /**
   * ä»ä¸Šä¸‹æ–‡æ¨å¯¼æ­¥éª¤è¾“å…¥å‚æ•°
   */
  private async inferStepInputFromContext(step: WorkflowStep, state: EnhancedWorkflowState): Promise<any> {
    // ğŸ”§ ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å‚æ•°æ¨å¯¼ï¼Œé˜²æ­¢ç”Ÿæˆå ä½ç¬¦
    const lastResult = state.dataStore.lastResult;
    const action = step.action.toLowerCase();
    
    if (!lastResult) {
      return {};
    }
    
    try {
      // ä½¿ç”¨LLMæ™ºèƒ½æå–å®é™…æ•°æ®
      const inferencePrompt = `You are an expert data extraction assistant. Your task is to extract ACTUAL DATA from the previous step result to create appropriate input parameters for the next action.

**Previous Step Result:**
${JSON.stringify(lastResult, null, 2)}

**Next Action:** ${step.action}
**MCP Service:** ${step.mcp}

**CRITICAL RULES:**
1. **NEVER use placeholders or descriptions** - always extract ACTUAL DATA
2. **Extract real content** from the previous result
3. **If data exists**: Use it directly, never summarize or describe it
4. **If no relevant data**: Return empty object {}, never use descriptive text
5. **DO NOT generate** "[Insert Data Here]" or similar placeholders

**EXAMPLES OF CORRECT EXTRACTION:**
- If previous result contains "Hello world" â†’ use "Hello world"
- If previous result contains {"content": "Bitcoin up 5%"} â†’ use "Bitcoin up 5%"
- If previous result has array of data â†’ extract the relevant items
- If previous result has URLs/links â†’ include them

**EXAMPLES OF WRONG BEHAVIOR:**
- NEVER: Use placeholder text or templates
- NEVER: Use descriptive summaries instead of actual data
- NEVER: Use generic examples or sample data
- NEVER: Use incomplete or missing information
- NEVER: Use data that doesn't match the action requirements

**SMART EXTRACTION RULES:**
- For tweet/post actions: Extract actual text content
- For search actions: Extract search terms or queries
- For publish actions: Extract content to be published OR draft_id if available
- For data analysis: Extract the actual data points
- For publish_draft actions: Extract draft_id from previous create_draft result, if no draft_id available, return empty object

**OUTPUT FORMAT:**
Return a JSON object with the extracted parameters:
{
  "parameterName": "actual_extracted_value"
}

Extract the actual data now:`;

      const response = await this.llm.invoke([new SystemMessage(inferencePrompt)]);
      const responseText = response.content.toString().trim();
      
      // è§£æLLMå“åº”
      let extractedParams;
      try {
        const jsonMatch = responseText.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          extractedParams = JSON.parse(jsonMatch[0]);
        } else {
          extractedParams = JSON.parse(responseText);
        }
      } catch (parseError) {
        logger.warn(`âŒ Failed to parse LLM inference response, using fallback: ${parseError}`);
        extractedParams = this.fallbackInferenceLogic(step, lastResult);
      }
      
      logger.info(`ğŸ” Inferred input parameters: ${JSON.stringify(extractedParams, null, 2)}`);
      return extractedParams;
      
    } catch (error) {
      logger.error(`âŒ Smart inference failed, using fallback logic: ${error}`);
      return this.fallbackInferenceLogic(step, lastResult);
    }
  }
  
  /**
   * é™çº§æ¨å¯¼é€»è¾‘
   */
  private fallbackInferenceLogic(step: WorkflowStep, lastResult: any): any {
    const action = step.action.toLowerCase();
    
    // ç®€å•çš„é™çº§æ¨å¯¼é€»è¾‘
    if (lastResult && typeof lastResult === 'object') {
      if (action.includes('tweet') && lastResult.text) {
        return { content: lastResult.text };
      }
      if (action.includes('search') && lastResult.query) {
        return { query: lastResult.query };
      }
      if (action.includes('get') && lastResult.id) {
        return { id: lastResult.id };
      }
      if (action.includes('publish') && lastResult.content) {
        return { content: lastResult.content };
      }
      // é€šç”¨æƒ…å†µï¼šæå–ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å€¼
      const firstStringValue = this.extractFirstStringValue(lastResult);
      if (firstStringValue) {
        return { content: firstStringValue };
      }
    }
    
    return {};
  }
  
  /**
   * æå–å¯¹è±¡ä¸­ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„å­—ç¬¦ä¸²å€¼
   */
  private extractFirstStringValue(obj: any): string | null {
    if (typeof obj === 'string') {
      return obj;
    }
    
    if (typeof obj === 'object' && obj !== null) {
      for (const [key, value] of Object.entries(obj)) {
        if (typeof value === 'string' && value.length > 0 && !key.toLowerCase().includes('id')) {
          return value;
        }
      }
    }
    
    return null;
  }

  /**
   * æ™ºèƒ½æ¨æ–­å®é™…å·¥å…·åç§°ï¼šä½¿ç”¨LLMå°†æè¿°æ€§æ–‡æœ¬è½¬æ¢ä¸ºå®é™…çš„MCPå·¥å…·åç§° (å‚è€ƒAgentå¼•æ“çš„é€šç”¨åšæ³•)
   */
  private async inferActualToolName(mcpName: string, action: string, input: any, userId: string): Promise<string> {
    try {
      // è·å–MCPçš„å¯ç”¨å·¥å…·åˆ—è¡¨
      const tools = await this.mcpManager.getTools(mcpName, userId);
      
      if (!tools || tools.length === 0) {
        logger.warn(`ğŸ” No tools found for MCP ${mcpName}, using original action: ${action}`);
        return action;
      }
      
      const toolNames = tools.map((tool: any) => tool.name);
      logger.info(`ğŸ” Available tools for ${mcpName}: ${toolNames.join(', ')}`);
      
      // 1. é¦–å…ˆæ£€æŸ¥actionæ˜¯å¦å·²ç»æ˜¯æœ‰æ•ˆçš„å·¥å…·åç§°
      if (toolNames.includes(action)) {
        logger.info(`âœ… Action "${action}" is already a valid tool name`);
        return action;
      }
      
      // 2. ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å·¥å…·åç§°æ¨æ–­ (é€šç”¨æ–¹æ³•ï¼Œå‚è€ƒAgentå¼•æ“)
      const toolInferencePrompt = `You are an expert tool name matcher. The requested action "${action}" needs to be mapped to an actual tool name from MCP service "${mcpName}".

CONTEXT:
- Requested action: ${action}
- Input parameters: ${JSON.stringify(input, null, 2)}
- MCP Service: ${mcpName}
- Available tools with descriptions:
${tools.map((tool: any) => {
  return `
Tool: ${tool.name}
Description: ${tool.description || 'No description'}
Input Schema: ${JSON.stringify(tool.inputSchema || {}, null, 2)}
`;
}).join('\n')}

MATCHING PRINCIPLES:
1. **Find functionally equivalent tool**: Select the tool that can accomplish the same objective as the requested action
2. **Consider semantic meaning**: Match based on functionality, not just text similarity
3. **Use exact tool names**: Return the exact tool name from the available list
4. **Prioritize best match**: Choose the most appropriate tool for the requested action

OUTPUT FORMAT:
Return a JSON object with exactly this structure:
{
  "toolName": "exact_tool_name_from_available_list",
  "reasoning": "why this tool was selected for the requested action"
}

Select the best matching tool now:`;

      const response = await this.llm.invoke([new SystemMessage(toolInferencePrompt)]);
      
      try {
        const responseText = response.content.toString().trim();
        logger.info(`ğŸ” === LLM Tool Inference Debug ===`);
        logger.info(`ğŸ” Original Action: ${action}`);
        logger.info(`ğŸ” Raw LLM Response: ${responseText}`);
        
        // ğŸ”§ ä½¿ç”¨Agentå¼•æ“ç›¸åŒçš„JSONæ¸…ç†é€»è¾‘
        let cleanedJson = responseText;
        
        // ç§»é™¤Markdownä»£ç å—æ ‡è®°
        cleanedJson = cleanedJson.replace(/```json\n?/g, '').replace(/```\n?/g, '');
        
        // ğŸ”§ ä½¿ç”¨Agentå¼•æ“çš„JSONæå–é€»è¾‘
        const extractedJson = this.extractCompleteJson(cleanedJson);
        if (extractedJson) {
          cleanedJson = extractedJson;
        }
        
        const inference = JSON.parse(cleanedJson);
        const selectedTool = inference.toolName;
        
        if (selectedTool && toolNames.includes(selectedTool)) {
          logger.info(`âœ… LLM selected tool: ${selectedTool} (${inference.reasoning})`);
          return selectedTool;
        } else {
          logger.warn(`âš ï¸ LLM selected invalid tool: ${selectedTool}, falling back to first available`);
        }
        
      } catch (parseError) {
        logger.error(`âŒ Failed to parse LLM tool inference response: ${response.content}`);
        logger.error(`âŒ Parse error: ${parseError}`);
      }
      
      // 3. å¦‚æœLLMæ¨æ–­å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥å…·ä½œä¸ºé»˜è®¤å€¼
      if (toolNames.length > 0) {
        logger.warn(`ğŸ” Using first available tool as fallback: ${toolNames[0]}`);
        return toolNames[0];
      }
      
      // 4. æœ€åçš„fallback
      logger.warn(`ğŸ” No tools available for MCP ${mcpName}, using original action: ${action}`);
      return action;
      
    } catch (error) {
      logger.error(`âŒ Error inferring tool name for ${mcpName}.${action}:`, error);
      return action; // å¦‚æœæ¨æ–­å¤±è´¥ï¼Œè¿”å›åŸå§‹action
    }
  }

  /**
   * æå–å®Œæ•´JSONå¯¹è±¡ (ä»Agentå¼•æ“å¤åˆ¶)
   */
  private extractCompleteJson(text: string): string | null {
    // æŸ¥æ‰¾ç¬¬ä¸€ä¸ª '{' çš„ä½ç½®
    const startIndex = text.indexOf('{');
    if (startIndex === -1) {
      return null;
    }
    
    // ä» '{' å¼€å§‹ï¼Œæ‰‹åŠ¨åŒ¹é…å¤§æ‹¬å·ä»¥æ‰¾åˆ°å®Œæ•´çš„JSONå¯¹è±¡
    let braceCount = 0;
    let inString = false;
    let escapeNext = false;
    
    for (let i = startIndex; i < text.length; i++) {
      const char = text[i];
      
      if (escapeNext) {
        escapeNext = false;
        continue;
      }
      
      if (char === '\\' && inString) {
        escapeNext = true;
        continue;
      }
      
      if (char === '"' && !escapeNext) {
        inString = !inString;
        continue;
      }
      
      if (!inString) {
        if (char === '{') {
          braceCount++;
        } else if (char === '}') {
          braceCount--;
          
          // å½“å¤§æ‹¬å·è®¡æ•°ä¸º0æ—¶ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†å®Œæ•´çš„JSONå¯¹è±¡
          if (braceCount === 0) {
            const jsonString = text.substring(startIndex, i + 1);
            logger.info(`ğŸ”§ Extracted complete JSON: ${jsonString}`);
            return jsonString;
          }
        }
      }
    }
    
    // å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„JSONå¯¹è±¡ï¼Œè¿”å›null
    logger.warn(`âš ï¸ Could not find complete JSON object`);
    return null;
    }

  /**
   * ğŸ§  æ™ºèƒ½ä»»åŠ¡å¤æ‚åº¦åˆ†æï¼ˆé’ˆå¯¹ä»»åŠ¡å¼•æ“ä¼˜åŒ–ï¼‰
   */
  private async analyzeTaskComplexity(
    query: string, 
    workflowSteps: number
  ): Promise<{
    type: 'simple_query' | 'medium_task' | 'complex_workflow';
    recommendedObservation: 'fast' | 'balanced' | 'thorough';
    shouldCompleteEarly: boolean;
    reasoning: string;
  }> {
    try {
      // ğŸ” åŸºäºæ¨¡å¼çš„å¿«é€Ÿåˆ†æ
      const quickAnalysis = this.quickTaskComplexityAnalysis(query, workflowSteps);
      if (quickAnalysis) {
        return quickAnalysis;
      }

      // ğŸ§  LLMæ·±åº¦åˆ†æï¼ˆç”¨äºè¾¹ç¼˜æƒ…å†µï¼‰
      const analysisPrompt = `Analyze the task complexity for workflow execution and recommend observation strategy.

**User Query**: "${query}"
**Workflow Steps**: ${workflowSteps} steps

**Task Types:**
1. **SIMPLE_QUERY** (Direct data requests):
   - "Show me...", "Get current...", "What is..."
   - Single data point requests
   - Basic information lookup
   - Observation: Fast - complete after first success

2. **MEDIUM_TASK** (Multi-step operations):
   - "Compare X and Y", "Analyze trends"
   - Data processing and basic analysis
   - Sequential operations with dependencies
   - Observation: Balanced - observe key checkpoints

3. **COMPLEX_WORKFLOW** (Comprehensive tasks):
   - Multi-source analysis with transformations
   - Complex decision workflows
   - Extensive data processing chains
   - Observation: Thorough - observe every step

**OUTPUT FORMAT (JSON only):**
{
  "type": "simple_query|medium_task|complex_workflow",
  "recommended_observation": "fast|balanced|thorough",
  "should_complete_early": true/false,
  "reasoning": "Brief explanation of complexity assessment"
}`;

      const response = await this.llm.invoke([new SystemMessage(analysisPrompt)]);
      const content = response.content as string;
      
      // è§£æLLMå“åº”
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        return {
          type: parsed.type || 'medium_task',
          recommendedObservation: parsed.recommended_observation || 'balanced',
          shouldCompleteEarly: parsed.should_complete_early || false,
          reasoning: parsed.reasoning || 'LLM analysis completed'
        };
      }
    } catch (error) {
      logger.warn(`Task complexity analysis failed: ${error}`);
    }

    // é»˜è®¤ä¸­ç­‰å¤æ‚åº¦
    return {
      type: 'medium_task',
      recommendedObservation: 'balanced',
      shouldCompleteEarly: false,
      reasoning: 'Default complexity analysis'
    };
  }

  /**
   * ğŸ” å¿«é€Ÿæ¨¡å¼åŒ¹é…å¤æ‚åº¦åˆ†æï¼ˆé’ˆå¯¹ä»»åŠ¡å¼•æ“ï¼‰
   */
  private quickTaskComplexityAnalysis(
    query: string, 
    workflowSteps: number
  ): {
    type: 'simple_query' | 'medium_task' | 'complex_workflow';
    recommendedObservation: 'fast' | 'balanced' | 'thorough';
    shouldCompleteEarly: boolean;
    reasoning: string;
  } | null {
    const lowerQuery = query.toLowerCase().trim();

    // ğŸŸ¢ ç®€å•æŸ¥è¯¢æ¨¡å¼ (1-2 steps, fast completion)
    const simplePatterns = [
      /^(show me|get|fetch|what is|current|latest)\s/,
      /^(how much|how many|price of|value of)\s/,
      /^(status of|info about|details of)\s/,
      /\b(index|price|value|status|information)\s*(of|for)?\s*\w+$/,
      /^(get current|show current|fetch latest)\s/
    ];

    if (simplePatterns.some(pattern => pattern.test(lowerQuery)) || workflowSteps <= 2) {
      return {
        type: 'simple_query',
        recommendedObservation: 'fast',
        shouldCompleteEarly: true,
        reasoning: 'Direct data query - fast completion after first success'
      };
    }

    // ğŸŸ¡ ä¸­ç­‰ä»»åŠ¡æ¨¡å¼ (3-5 steps, balanced observation)
    const mediumPatterns = [
      /\b(compare|analyze|calculate|process)\b/,
      /\b(then|after|next|followed by)\b/,
      /\b(both|all|multiple|several)\b/,
      /\band\s+\w+\s+(also|too|as well)/,
      /\b(summary|report|overview)\b/
    ];

    if (mediumPatterns.some(pattern => pattern.test(lowerQuery)) || (workflowSteps >= 3 && workflowSteps <= 5)) {
      return {
        type: 'medium_task',
        recommendedObservation: 'balanced',
        shouldCompleteEarly: false,
        reasoning: 'Multi-step task requiring balanced observation'
      };
    }

    // ğŸ”´ å¤æ‚å·¥ä½œæµæ¨¡å¼ (6+ steps, thorough observation)
    const complexPatterns = [
      /\b(workflow|pipeline|process.*step)\b/,
      /\b(first.*then.*finally|step.*step.*step)\b/,
      /\b(comprehensive|detailed|thorough)\s+(analysis|report|study)\b/,
      /\b(multiple.*and.*then)\b/,
      /\b(optimize|automate|integrate)\b/
    ];

    if (complexPatterns.some(pattern => pattern.test(lowerQuery)) || workflowSteps > 5 || lowerQuery.length > 100) {
      return {
        type: 'complex_workflow',
        recommendedObservation: 'thorough',
        shouldCompleteEarly: false,
        reasoning: 'Complex multi-step workflow requiring thorough observation'
      };
    }

    return null; // éœ€è¦LLMæ·±åº¦åˆ†æ
  }

/**
 * ğŸ§  æ–°å¢ï¼šåŠ¨æ€è§„åˆ’é˜¶æ®µï¼ˆå‚è€ƒAgentå¼•æ“ï¼Œä½¿ä»»åŠ¡å¼•æ“ä¹Ÿå…·å¤‡æ™ºèƒ½è§„åˆ’èƒ½åŠ›ï¼‰
 */
private async taskDynamicPlanningPhase(
  state: EnhancedWorkflowState,
  currentContext: string
): Promise<{
  success: boolean;
  adaptedSteps?: Array<{
    step: number;
    mcp: string;
    action: string;
    input?: any;
    reasoning?: string;
  }>;
  error?: string;
}> {
  try {
    // ğŸ”§ è·å–å½“å‰å¯ç”¨çš„MCPå’Œæ‰§è¡Œå†å²
    const availableMCPs = await this.getAvailableMCPsForPlanning(state.taskId);
    const executionHistory = this.buildExecutionHistory(state);
    
    const plannerPrompt = this.buildTaskPlannerPrompt(state, availableMCPs, currentContext, executionHistory);

    // ğŸ”„ ä½¿ç”¨LLMè¿›è¡ŒåŠ¨æ€è§„åˆ’
    const response = await this.llm.invoke([new SystemMessage(plannerPrompt)]);
    const adaptedSteps = this.parseTaskPlan(response.content.toString());

    return { success: true, adaptedSteps };
  } catch (error) {
    logger.error('Task dynamic planning failed:', error);
    return { 
      success: false, 
      error: error instanceof Error ? error.message : String(error) 
    };
  }
}

/**
 * ğŸ§  æ–°å¢ï¼šä»»åŠ¡è§‚å¯Ÿé˜¶æ®µï¼ˆå‚è€ƒAgentå¼•æ“ï¼Œæ™ºèƒ½åˆ†æå½“å‰è¿›åº¦å’Œè°ƒæ•´ç­–ç•¥ï¼‰
 */
private async taskObservationPhase(
  state: EnhancedWorkflowState,
  taskComplexity?: { type: string; recommendedObservation: string; shouldCompleteEarly: boolean; reasoning: string }
): Promise<{
  shouldContinue: boolean;
  shouldAdaptWorkflow: boolean;
  adaptationReason?: string;
  newObjective?: string;
}> {
  try {
    const observerPrompt = this.buildTaskObserverPrompt(state, taskComplexity);
    const response = await this.llm.invoke([new SystemMessage(observerPrompt)]);
    
    return this.parseTaskObservation(response.content.toString());
  } catch (error) {
    logger.error('Task observation failed:', error);
    return { 
      shouldContinue: true, 
      shouldAdaptWorkflow: false 
    };
  }
}

/**
 * ğŸ”§ æ„å»ºä»»åŠ¡è§„åˆ’æç¤ºè¯
 */
private buildTaskPlannerPrompt(
  state: EnhancedWorkflowState,
  availableMCPs: any[],
  currentContext: string,
  executionHistory: string
): string {
  return `You are an intelligent task workflow planner. Based on the current execution context and available tools, dynamically plan the optimal next steps.

**Current Task**: ${state.originalQuery}

**Execution Context**: ${currentContext}

**Available MCP Tools**:
${JSON.stringify(availableMCPs.map(mcp => ({
  name: mcp.name,
  description: mcp.description,
  capabilities: mcp.predefinedTools?.map((tool: any) => tool.name) || []
})), null, 2)}

**Previous Execution History**:
${executionHistory}

**Current Workflow Progress**: ${state.completedSteps}/${state.totalSteps} steps completed

**Instructions**:
1. Analyze what has been accomplished so far
2. Identify what still needs to be done to complete the original task
3. Plan the optimal next steps using available MCP tools
4. Consider efficiency and logical flow
5. Adapt based on previous results

Respond with valid JSON in this exact format:
{
  "analysis": "Brief analysis of current progress and what's needed",
  "adapted_steps": [
    {
      "step": 1,
      "mcp": "mcp_name",
      "action": "Clear description of what this step will accomplish",
      "input": {"actual": "parameters"},
      "reasoning": "Why this step is needed now"
    }
  ],
  "planning_reasoning": "Detailed explanation of the planning logic"
}`;
}

/**
 * ğŸ”§ æ„å»ºä»»åŠ¡è§‚å¯Ÿæç¤ºè¯ï¼ˆæ™ºèƒ½å¤æ‚åº¦æ„ŸçŸ¥ï¼‰
 */
private buildTaskObserverPrompt(
  state: EnhancedWorkflowState,
  taskComplexity?: { type: string; recommendedObservation: string; shouldCompleteEarly: boolean; reasoning: string }
): string {
  return `You are analyzing whether sufficient work has been completed to answer the user's question.

## ğŸ“‹ USER'S ORIGINAL QUESTION
"${state.originalQuery}"

## ğŸ“Š EXECUTION ANALYSIS

### Execution History
${state.executionHistory.map(step => `
**Step ${step.stepNumber}**: ${step.action}
- Status: ${step.success ? 'âœ… Success' : 'âŒ Failed'}
- MCP: ${step.mcpName}
- Data Retrieved: ${step.success && step.result ? 'Yes' : 'No'}
${step.success && step.result ? `- Raw Result Data: ${JSON.stringify(step.result, null, 2)}` : ''}
${step.error ? `- Error: ${step.error}` : ''}
`).join('\n')}

### Critical Analysis Required
**ğŸ” DETAILED COMPARISON NEEDED**:

1. **Parse the user's original request** - What EXACTLY did they ask for?
2. **Analyze the collected data** - What have we actually obtained so far?
3. **Gap Analysis** - What is missing between request and current data?

**ğŸš¨ CRITICAL**: For requests mentioning multiple items/users/targets:
- Count how many were requested vs how many we have data for
- Example: User asks for "A, B, C, D" but we only have data for "A, B" â†’ INCOMPLETE!

## ğŸ§  INTELLIGENT ANALYSIS REQUIRED

**Critical Questions**: 
1. Does the collected data contain the specific information requested by the user?
2. Can you identify and extract the exact answer from the available data?
3. Is the data recent, relevant, and sufficient in scope?

**For "${state.originalQuery}"**:
**INTELLIGENT ANALYSIS**:
Analyze the user's original request: "${state.originalQuery}"

Ask yourself:
1. What EXACTLY did the user ask for?
2. What are the KEY COMPONENTS that must be completed?
3. Are there multiple parts/targets/items mentioned?
4. What is the END GOAL the user wants to achieve?
5. Has that end goal been fully achieved with current data/actions?

**CRITICAL THINKING** (Be extremely thorough):
- Count EXACTLY what the user requested vs what we have
- Don't assume "some data = complete" - verify COMPLETENESS
- For multi-target requests: ALL targets must be processed
- Examine each result summary above: does it contain the requested information?
- Ask: "Would a reasonable person consider this request fully satisfied?"
- If user asked for data on 8 users but we only have 2 â†’ CLEARLY INCOMPLETE
- If user asked for posting/publishing but only collected data â†’ INCOMPLETE
- Use logical reasoning: partial completion â‰  task completion

## ğŸ¯ DECISION LOGIC

**ğŸ§  USE YOUR INTELLIGENCE TO JUDGE**:
- Read the user's original request carefully
- Look at what has been accomplished so far
- Consider whether a reasonable person would say "this request has been fulfilled"
- Don't be overly strict, but also don't accept partial completion as full success
- If the user asked for multiple things, check if ALL of them have been addressed
- If the user asked for an action (like posting), check if that action actually happened

**DECISION GUIDELINES**:
âœ… Mark COMPLETE if: EVERY SINGLE item/user/target in the original request has been processed
âŒ Mark CONTINUE if: ANY item/user/target from the original request is missing

**ğŸš¨ MANDATORY CHECK**: 
- Count total items requested in original query
- Count total items successfully processed  
- If numbers don't match â†’ MUST continue
- Example: 8 users requested, 3 users processed â†’ 5 still missing â†’ CONTINUE!

**OUTPUT FORMAT (JSON only)**:
{
  "shouldContinue": true/false,
  "reasoning": "Focus on whether the specific user question can be answered with available data",
  "newObjective": "If continue, what specific missing information is needed?",
  "shouldAdaptWorkflow": false
}

**ğŸš¨ THINK LIKE A HUMAN**: 
Would a reasonable person consider this request fulfilled based on what has been accomplished? 
Use your intelligence and common sense to make the judgment.`;
}

/**
 * ğŸ”§ è§£æä»»åŠ¡è§„åˆ’ç»“æœ
 */
private parseTaskPlan(content: string): Array<{
  step: number;
  mcp: string;
  action: string;
  input?: any;
  reasoning?: string;
}> {
  try {
    const cleanedContent = content
      .replace(/```json\s*/g, '')
      .replace(/```\s*$/g, '')
      .trim();
    
    const parsed = JSON.parse(cleanedContent);
    return parsed.adapted_steps || [];
  } catch (error) {
    logger.error('Failed to parse task plan:', error);
    return [];
  }
}

/**
 * ğŸ”§ è§£æä»»åŠ¡è§‚å¯Ÿç»“æœ
 */
private parseTaskObservation(content: string): {
  shouldContinue: boolean;
  shouldAdaptWorkflow: boolean;
  adaptationReason?: string;
  newObjective?: string;
} {
  try {
    let jsonText = content.trim();
    jsonText = jsonText.replace(/```json\s*|\s*```/g, '');
    jsonText = jsonText.replace(/```\s*|\s*```/g, '');
    
    const jsonMatch = jsonText.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      const parsed = JSON.parse(jsonMatch[0]);
      
      return {
        shouldContinue: parsed.shouldContinue !== false,
        shouldAdaptWorkflow: parsed.shouldAdaptWorkflow === true,
        adaptationReason: parsed.reasoning,
        newObjective: parsed.newObjective
      };
    }
  } catch (error) {
    logger.warn(`Task observation parsing failed: ${error}`);
  }
  
  // é™çº§æ–¹æ¡ˆ
  return { 
    shouldContinue: true, 
    shouldAdaptWorkflow: false 
  };
}

/**
 * ğŸ”§ è·å–å¯ç”¨äºè§„åˆ’çš„MCPåˆ—è¡¨
 */
private async getAvailableMCPsForPlanning(taskId: string): Promise<any[]> {
  try {
    const task = await this.taskService.getTaskById(taskId);
    if (task?.mcpWorkflow?.mcps) {
      return task.mcpWorkflow.mcps;
    }
    return [];
  } catch (error) {
    logger.error('Failed to get available MCPs for planning:', error);
    return [];
  }
}

  /**
   * ğŸ”§ æ„å»ºæ‰§è¡Œå†å²æ‘˜è¦
   */
  private buildExecutionHistory(state: EnhancedWorkflowState): string {
    if (state.executionHistory.length === 0) {
      return 'No previous execution history.';
    }
    
    return state.executionHistory
      .map(step => `Step ${step.stepNumber}: ${step.action} -> ${step.success ? 'Success' : 'Failed'}`)
      .join('\n');
  }

  /**
   * ğŸ”§ æ„å»ºå½“å‰æ‰§è¡Œä¸Šä¸‹æ–‡
   */
  private buildCurrentContext(state: EnhancedWorkflowState): string {
    const completedSteps = state.executionHistory.filter(step => step.success);
    const failedSteps = state.executionHistory.filter(step => !step.success);
    
    let context = `Current execution context for task: ${state.originalQuery}\n\n`;
    
    // è¿›åº¦æ¦‚è§ˆ
    context += `Progress Overview:\n`;
    context += `- Completed: ${state.completedSteps}/${state.totalSteps} steps\n`;
    context += `- Failed: ${state.failedSteps} steps\n`;
    context += `- Current step index: ${state.currentStepIndex}\n\n`;
    
    // å·²å®Œæˆçš„æ­¥éª¤å’Œç»“æœ
    if (completedSteps.length > 0) {
      context += `Successfully completed steps:\n`;
      completedSteps.forEach(step => {
        const resultSummary = typeof step.result === 'string' 
          ? step.result.substring(0, 100) + '...'
          : JSON.stringify(step.result).substring(0, 100) + '...';
        context += `- Step ${step.stepNumber}: ${step.action} -> ${resultSummary}\n`;
      });
      context += '\n';
    }
    
    // å¤±è´¥çš„æ­¥éª¤
    if (failedSteps.length > 0) {
      context += `Failed steps:\n`;
      failedSteps.forEach(step => {
        context += `- Step ${step.stepNumber}: ${step.action} -> Error: ${step.error}\n`;
      });
      context += '\n';
    }
    
    // å¯ç”¨æ•°æ®
    if (Object.keys(state.dataStore).length > 0) {
      context += `Available data in context:\n`;
      Object.keys(state.dataStore).forEach(key => {
        context += `- ${key}: ${typeof state.dataStore[key]}\n`;
      });
    }
    
    return context;
  }

/**
 * ğŸ”§ æ–°å¢ï¼šæµå¼æ ¼å¼åŒ–ä»»åŠ¡ç»“æœï¼ˆå‚è€ƒAgentå¼•æ“å®ç°ï¼‰
 */
  private async *formatAndStreamTaskResult(
    rawResult: any,
    mcpName: string,
    toolName: string
  ): AsyncGenerator<string, void, unknown> {
    try {
      // ğŸ”§ çº¯ç²¹çš„æ ¼å¼è½¬æ¢ï¼šJSON â†’ Markdownï¼ˆæ™ºèƒ½é•¿åº¦æ§åˆ¶ï¼‰
      const dataString = typeof rawResult === 'string' ? rawResult : JSON.stringify(rawResult, null, 2);
      const isLongData = dataString.length > 3000; // è¶…è¿‡3000å­—ç¬¦è®¤ä¸ºæ˜¯é•¿æ•°æ®
      
      const formatPrompt = `Convert this JSON data to clean, readable Markdown format. Output the formatted Markdown directly without any code blocks or wrappers.

**Data to format:**
${dataString}

**Formatting rules:**
- Convert JSON structure to clear Markdown
- Use tables for object data when helpful
- Use lists for arrays
- Make long numbers readable with commas
- Output the formatted Markdown directly
- DO NOT wrap in code blocks or backticks
- DO NOT add explanations or descriptions

${isLongData ? `
**IMPORTANT - Data Length Control:**
This is a large dataset. Apply smart filtering:
- Show only the most important/commonly used fields
- For blockchain data: show hash, number, gasUsed, gasLimit, miner, timestamp, parentHash
- Skip verbose fields like logsBloom, extraData, mix_hash unless they contain short meaningful values
- For large objects: show top 10-15 most relevant fields
- Always prioritize user-actionable or identifying information
- Keep the output concise and focused
` : `
**Standard formatting:**
- Keep ALL original data values
- Format all available fields
`}
- ONLY return the formatted data`;


      // ä½¿ç”¨æµå¼LLMç”Ÿæˆæ ¼å¼åŒ–ç»“æœ
      const stream = await this.llm.stream([new SystemMessage(formatPrompt)]);

      for await (const chunk of stream) {
        if (chunk.content) {
          yield chunk.content as string;
        }
      }
    } catch (error) {
      logger.error(`Failed to format task result:`, error);
      // é™çº§å¤„ç†ï¼šè¿”å›åŸºæœ¬æ ¼å¼åŒ–
      const fallbackResult = `### ${toolName} æ‰§è¡Œç»“æœ\n\n\`\`\`json\n${JSON.stringify(rawResult, null, 2)}\n\`\`\``;
      yield fallbackResult;
    }
  }

  /**
   * ç”Ÿæˆæ ¼å¼åŒ–ç»“æœï¼ˆéæµå¼ï¼Œç”¨äºå­˜å‚¨ï¼‰
   */
  private async generateFormattedResult(rawResult: any, mcpName: string, action: string): Promise<string> {
    try {
      const dataString = JSON.stringify(rawResult, null, 2);
      // ğŸ”§ ç§»é™¤é•¿æ•°æ®åˆ¤æ–­é™åˆ¶ï¼Œå…è®¸å¤„ç†ä»»æ„é•¿åº¦çš„æ•°æ®
      const isLongData = false; // dataString.length > 3000; // ç§»é™¤3000å­—ç¬¦é™åˆ¶
      
      const prompt = `Convert this JSON data to clean, readable Markdown format. Output the formatted Markdown directly without any code blocks or wrappers.

**Data to format:**
${dataString}

**Formatting rules:**
- Convert JSON structure to clear Markdown
- Use tables for object data when helpful
- Use lists for arrays
- Make long numbers readable with commas
- Output the formatted Markdown directly
- DO NOT wrap in code blocks or backticks
- DO NOT add explanations or descriptions

${isLongData ? `
**IMPORTANT - Data Length Control:**
This is a large dataset. Apply smart filtering:
- Show only the most important/commonly used fields
- For blockchain data: show hash, number, gasUsed, gasLimit, miner, timestamp, parentHash
- Skip verbose fields like logsBloom, extraData, mix_hash unless they contain short meaningful values
- For large objects: show top 10-15 most relevant fields
- Always prioritize user-actionable or identifying information
- Keep the output concise and focused
` : `
**Standard formatting:**
- Keep ALL original data values
- Format all available fields
`}
- ONLY return the formatted data`;

      const response = await this.llm.invoke([new SystemMessage(prompt)]);
      return response.content as string;
    } catch (error) {
      logger.error('Failed to format result:', error);
      return JSON.stringify(rawResult, null, 2);
    }
  }

  /**
   * æ£€æŸ¥æ˜¯å¦ä¸ºMCPè¿æ¥é”™è¯¯
   */
  private isMCPConnectionError(error: string): boolean {
    const lowerError = error.toLowerCase();
    return lowerError.includes('mcp') || 
           lowerError.includes('connection') || 
           lowerError.includes('auth') ||
           lowerError.includes('not connected');
  }

  /**
   * ç”Ÿæˆå·¥ä½œæµæœ€ç»ˆç»“æœ
   */
  private generateWorkflowFinalResult(state: EnhancedWorkflowState): string {
    const successRate = Math.round((state.completedSteps / state.totalSteps) * 100);
    
    let summary = `Workflow execution completed with ${successRate}% success rate.\n\n`;
    summary += `**Execution Summary:**\n`;
    summary += `- Total Steps: ${state.totalSteps}\n`;
    summary += `- Completed: ${state.completedSteps}\n`;
    summary += `- Failed: ${state.failedSteps}\n\n`;
    
    if (state.completedSteps > 0) {
      summary += `**Successful Steps:**\n`;
      state.executionHistory
        .filter(entry => entry.success)
        .forEach(entry => {
          summary += `- Step ${entry.stepNumber}: ${entry.mcpName}.${entry.action} âœ…\n`;
        });
    }
    
    if (state.failedSteps > 0) {
      summary += `\n**Failed Steps:**\n`;
      state.executionHistory
        .filter(entry => !entry.success)
        .forEach(entry => {
          summary += `- Step ${entry.stepNumber}: ${entry.mcpName}.${entry.action} âŒ (${entry.error})\n`;
        });
    }
    
    return summary;
  }



  /**
   * ä¿å­˜æ­¥éª¤åŸå§‹ç»“æœæ¶ˆæ¯
   */
  private async saveStepRawResult(taskId: string, stepNumber: number, step: WorkflowStep, rawResult: any, actualArgs: any, toolType: string, mcpName: string | null, expectedOutput: string, reasoning: string, actualToolName?: string): Promise<void> {
    try {
      const task = await this.taskService.getTaskById(taskId);
      if (task.conversationId) {
        // ğŸ”§ ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´çš„contentæ ¼å¼å’Œmetadataç»“æ„
        const toolName = actualToolName || step.action;
        const rawContent = `WorkflowEngine Step ${stepNumber} Raw Result: ${toolName}

${JSON.stringify(rawResult, null, 2)}`;

        await messageDao.createMessage({
          conversationId: task.conversationId,
          content: rawContent,
          type: MessageType.ASSISTANT,
          intent: MessageIntent.TASK,
          taskId,
          metadata: {
            stepType: MessageStepType.EXECUTION,
            stepNumber: stepNumber,
            stepName: toolName,
            taskPhase: 'execution',
            contentType: 'raw_result',
            agentName: 'WorkflowEngine',
            isComplete: true,
            toolDetails: {
              toolType: toolType,
              toolName: toolName,
              mcpName: mcpName,
              args: actualArgs || step.input || {},
              expectedOutput: expectedOutput,
              reasoning: reasoning,
              timestamp: new Date().toISOString()
            },
            executionDetails: {
              rawResult: rawResult,
              success: true,
              processingInfo: {
                originalDataSize: JSON.stringify(rawResult).length,
                processingTime: new Date().toISOString()
              }
            }
          }
        });

        await conversationDao.incrementMessageCount(task.conversationId);
      }
    } catch (error) {
      logger.error(`Failed to save workflow step raw result:`, error);
    }
  }

  /**
   * ä¿å­˜æ­¥éª¤æ ¼å¼åŒ–ç»“æœæ¶ˆæ¯
   */
  private async saveStepFormattedResult(taskId: string, stepNumber: number, step: WorkflowStep, formattedResult: string, actualArgs: any, toolType: string, mcpName: string | null, expectedOutput: string, reasoning: string, actualToolName?: string): Promise<void> {
    try {
      const task = await this.taskService.getTaskById(taskId);
      if (task.conversationId) {
        // ğŸ”§ ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´çš„contentæ ¼å¼å’Œmetadataç»“æ„
        const toolName = actualToolName || step.action;
        const resultType = toolType === 'llm' ? 'LLM Result' : 'Formatted Result';
        const formattedContent = `WorkflowEngine Step ${stepNumber} ${resultType}: ${toolName}

${formattedResult}`;

        await messageDao.createMessage({
          conversationId: task.conversationId,
          content: formattedContent,
          type: MessageType.ASSISTANT,
          intent: MessageIntent.TASK,
          taskId,
          metadata: {
            stepType: MessageStepType.EXECUTION,
            stepNumber: stepNumber,
            stepName: toolName,
            taskPhase: 'execution',
            contentType: 'formatted_result',
            agentName: 'WorkflowEngine',
            isComplete: true,
            toolDetails: {
              toolType: toolType,
              toolName: toolName,
              mcpName: mcpName,
              args: actualArgs || step.input || {},
              expectedOutput: expectedOutput,
              reasoning: reasoning,
              timestamp: new Date().toISOString()
            },
            executionDetails: {
              formattedResult: formattedResult,
              success: true,
              processingInfo: {
                formattedDataSize: formattedResult.length,
                processingTime: new Date().toISOString()
              }
            }
          }
        });

        await conversationDao.incrementMessageCount(task.conversationId);
      }
    } catch (error) {
      logger.error(`Failed to save workflow step formatted result:`, error);
    }
  }

  /**
   * ä¿å­˜ä»»åŠ¡æœ€ç»ˆç»“æœ
   */
  private async saveWorkflowFinalResult(taskId: string, state: EnhancedWorkflowState, finalResult: string): Promise<void> {
    try {
      const task = await this.taskService.getTaskById(taskId);
      if (task.conversationId) {
        await messageDao.createMessage({
          conversationId: task.conversationId,
          content: `Workflow Final Result:\n\n${finalResult}`,
          type: MessageType.ASSISTANT,
          intent: MessageIntent.TASK,
          taskId,
          metadata: {
            stepType: MessageStepType.SUMMARY,
            stepName: 'Workflow Completion',
            taskPhase: 'completion',
            isComplete: true,
            executionSummary: {
              totalSteps: state.totalSteps,
              completedSteps: state.completedSteps,
              failedSteps: state.failedSteps,
              successRate: Math.round((state.completedSteps / state.totalSteps) * 100)
            }
          }
        });
        await conversationDao.incrementMessageCount(task.conversationId);
      }
    } catch (error) {
      logger.error('Failed to save workflow final result:', error);
    }
  }

  /**
   * ğŸ¯ æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²æ”¶é›†è¶³å¤Ÿæ•°æ®ï¼ˆå‚è€ƒAgentå¼•æ“çš„ç›´æ¥åˆ¤æ–­æ–¹æ³•ï¼‰
   */
  private async hasTaskCollectedSufficientData(state: EnhancedWorkflowState): Promise<boolean> {
    // åŸºäºæ•°æ®å­˜å‚¨å’Œæ‰§è¡Œå†å²çš„å¿«é€Ÿåˆ¤æ–­
    const hasSuccessfulSteps = state.completedSteps > 0;
    const hasUsefulData = Object.keys(state.dataStore).length > 1; // é™¤äº† lastResult è¿˜æœ‰å…¶ä»–æ•°æ®
    
    return hasSuccessfulSteps && hasUsefulData;
  }

  /**
   * ğŸ¯ å¿«é€Ÿä»»åŠ¡å®Œæˆæ£€æŸ¥ï¼ˆå‚è€ƒAgentå¼•æ“çš„ç®€åŒ–åˆ¤æ–­é€»è¾‘ï¼‰
   */
  // ğŸ”§ ç§»é™¤ç¡¬ç¼–ç çš„å¿«é€Ÿå®Œæˆæ£€æŸ¥ï¼Œç°åœ¨ä½¿ç”¨æ™ºèƒ½è§‚å¯Ÿé˜¶æ®µ

  /**
   * ğŸ¯ ç®€åŒ–çš„å·¥ä½œæµé€‚åº”åˆ¤æ–­ï¼ˆå‡å°‘å¤æ‚åº¦ï¼‰
   */
  private async shouldAdaptWorkflow(state: EnhancedWorkflowState, currentStep: WorkflowStep): Promise<boolean> {
    // ç®€åŒ–çš„é€‚åº”åˆ¤æ–­ï¼šåªåœ¨è¿ç»­å¤±è´¥æ—¶é€‚åº”
    const recentFailures = state.executionHistory
      .slice(-2) // æœ€è¿‘2æ­¥
      .filter(step => !step.success);
    
    return recentFailures.length >= 2; // è¿ç»­2æ­¥å¤±è´¥æ‰é€‚åº”
  }

  /**
   * ğŸ”§ æ–°å¢ï¼šä¸º MCP è½¬æ¢å‚æ•°ï¼Œç¡®ä¿å‚æ•°åä¸å·¥å…· schema åŒ¹é…
   */
  private async convertParametersForMCP(mcpName: string, toolName: string, input: any, userId: string): Promise<any> {
    try {
      logger.info(`ğŸ”„ Converting parameters for MCP tool: ${mcpName}.${toolName}`);

      // è·å– MCP å·¥å…·çš„ schema
      const mcpTools = await this.mcpManager.getTools(mcpName, userId);
      const targetTool = mcpTools.find(tool => tool.name === toolName);
      
      if (!targetTool || !targetTool.inputSchema) {
        logger.info(`ğŸ” No schema found for ${mcpName}.${toolName}, returning original input`);
        return input;
      }

      // æ‰§è¡Œå‚æ•°åè½¬æ¢
      const convertedParams = this.preprocessParameterNames(input, targetTool.inputSchema);
      
      if (JSON.stringify(convertedParams) !== JSON.stringify(input)) {
        logger.info(`ğŸ”§ Parameters converted for ${mcpName}.${toolName}: ${JSON.stringify(input)} â†’ ${JSON.stringify(convertedParams)}`);
      }

      return convertedParams;

    } catch (error) {
      logger.error(`âŒ Parameter conversion failed for ${mcpName}.${toolName}:`, error);
      return input; // å›é€€åˆ°åŸå§‹è¾“å…¥
    }
  }

  /**
   * ğŸ”§ ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å‚æ•°è½¬æ¢ï¼Œé˜²æ­¢å ä½ç¬¦é—®é¢˜ï¼ˆå¤åˆ¶è‡ªAgentIntelligentEngineï¼‰
   */
  private async convertParametersWithLLM(toolName: string, originalArgs: any, mcpName: string, userId: string): Promise<{ toolName: string; params: any }> {
    try {
      logger.info(`ğŸ”„ Converting parameters for tool: ${toolName}`);

      // è·å–MCPå·¥å…·ä¿¡æ¯
      const mcpTools = await this.mcpManager.getTools(mcpName, userId);
      
      // ğŸ”§ æ–°å¢ï¼šé¢„å¤„ç†å‚æ•°åæ˜ å°„ï¼ˆcamelCase åˆ° snake_caseï¼‰
      const preprocessedArgs = this.preprocessParameterNames(originalArgs, mcpTools[0]?.inputSchema);
      if (JSON.stringify(preprocessedArgs) !== JSON.stringify(originalArgs)) {
        logger.info(`ğŸ”§ Parameter names preprocessed: ${JSON.stringify(originalArgs)} â†’ ${JSON.stringify(preprocessedArgs)}`);
      }

      // æ„å»ºæ™ºèƒ½å‚æ•°è½¬æ¢æç¤ºè¯
      const conversionPrompt = `You are an expert data transformation assistant. Your task is to intelligently transform parameters for MCP tool calls.

CONTEXT:
- Tool to call: ${toolName}
- MCP Service: ${mcpName}
- Input parameters: ${JSON.stringify(preprocessedArgs, null, 2)}
- Available tools with their schemas:
${mcpTools.map(tool => {
  const schema = tool.inputSchema || {};
  return `
Tool: ${tool.name}
Description: ${tool.description || 'No description'}
Input Schema: ${JSON.stringify(schema, null, 2)}
`;
}).join('\n')}

TRANSFORMATION PRINCIPLES:
1. **Use exact tool name**: ${toolName}
2. **Transform parameters**: Convert input into correct format for the tool
3. **CRITICAL: Use exact parameter names from the schema**: 
   - ALWAYS check the inputSchema and use the exact parameter names shown
   - For example, if the schema shows "text" as parameter name, use "text" NOT "tweet" or other variations
   - Match the exact property names shown in the inputSchema
4. **EXTRACT REAL DATA FROM CONTEXT**: 
   - If previous step results are available: Extract the ACTUAL CONTENT, never use placeholders
   - Look for real text, summaries, data in the previous results
   - Use the exact content from previous step, not descriptions like "Summary of @user's tweets"
   - Example: If previous result contains "Here's the summary: Bitcoin price is up 5%" â†’ use "Bitcoin price is up 5%"
5. **Handle missing data intelligently**: 
   - CRITICAL: NEVER use placeholders or descriptions - always extract ACTUAL DATA
   - For required data: Find and extract the real content from the input or previous results
   - If actual data exists: Use it directly, never summarize or describe it
   - If data is truly missing: Return empty string or null, never use descriptive text
   - DO NOT use hardcoded examples, templates, or placeholder descriptions

CRITICAL TWITTER RULES:
- Twitter has a HARD 280 character limit!
- Count ALL characters including spaces, emojis, URLs, hashtags
- If content is too long, you MUST:
  1. Use abbreviations (e.g., "w/" for "with")
  2. Shorten usernames (e.g., "@user" instead of full names)
  3. Remove less important details
  4. Keep URLs whenever possible as they are valuable references
  5. If URLs must be removed due to length, prefer keeping the most important ones
- For threads: First tweet should be <250 chars to leave room for thread numbering
- PRIORITY: Always try to include original tweet URLs when space allows

CRITICAL TWITTER WORKFLOW STRATEGY:
- **ALWAYS prefer create_draft_thread over publish_draft when you have content to publish**
- **ONLY use publish_draft if you have a CONFIRMED, REAL draft_id from previous create_draft result**
- **For publishing new content: Use create_draft_thread (it will auto-publish)**
- **For publishing existing draft: Use publish_draft with real draft_id**
- **If tool name is publish_draft but no real draft_id available: CHANGE to create_draft_thread**
- **Never use placeholder draft_ids like "12345", "draft_id", etc.**

OUTPUT FORMAT:
Return a JSON object with exactly this structure:
{
  "toolName": "ACTUAL_TOOL_NAME", /* Use create_draft_thread for new content OR publish_draft for existing drafts */
  "inputParams": { /* transformed parameters using EXACT parameter names from the tool's input schema */ },
  "reasoning": "brief explanation of tool selection and parameter transformation"
}

SMART TOOL SELECTION RULES:
- If original tool is "publish_draft" AND you have real content to publish BUT no valid draft_id: CHANGE toolName to "create_draft_thread"
- If original tool is "publish_draft" AND you have a real draft_id from previous step: KEEP toolName as "publish_draft"
- If original tool is "create_draft_thread": KEEP toolName as "create_draft_thread"
- NEVER use placeholder draft_ids - if you don't have a real one, switch to create_draft_thread

CRITICAL CONTENT EXTRACTION:
- When previous step results contain actual content: EXTRACT THE REAL TEXT, never use placeholders
- Example: If previous contains "Summary: Bitcoin is trending up 5%" â†’ use "Bitcoin is trending up 5%"
- NEVER use "[Insert summary here]" or "Latest tweet content from @user" - extract actual content!
- ALWAYS extract and include tweet URLs/links when available in the source data
- Format URLs efficiently to save characters (use bit.ly style if needed)

IMPORTANT: Always use exact parameter names from the inputSchema, ensure Twitter content is under 280 characters, and EXTRACT REAL CONTENT from previous results!

Transform the data now:`;

      const response = await this.llm.invoke([new SystemMessage(conversionPrompt)]);

      let conversion;
      try {
        const responseText = response.content.toString().trim();
        logger.info(`ğŸ” === LLM Parameter Conversion Debug ===`);
        logger.info(`ğŸ” Raw LLM Response: ${responseText}`);
        
        // æ¸…ç†JSONå“åº”
        let cleanedJson = responseText;
        cleanedJson = cleanedJson.replace(/```json\n?/g, '').replace(/```\n?/g, '');
        
        // æå–å®Œæ•´JSON
        const extractedJson = this.extractCompleteJson(cleanedJson);
        if (extractedJson) {
          cleanedJson = extractedJson;
        }
        
        conversion = JSON.parse(cleanedJson);
        logger.info(`ğŸ” Parsed Conversion: ${JSON.stringify(conversion, null, 2)}`);
      } catch (parseError) {
        logger.error(`âŒ Failed to parse parameter conversion response: ${response.content}`);
        logger.error(`âŒ Parse error: ${parseError}`);
        logger.info(`ğŸ” Falling back to preprocessedArgs: ${JSON.stringify(preprocessedArgs, null, 2)}`);
        return { toolName, params: preprocessedArgs }; // å›é€€åˆ°é¢„å¤„ç†åçš„å‚æ•°
      }

      const convertedParams = conversion.inputParams || preprocessedArgs;
      const finalToolName = conversion.toolName || toolName; // LLMå¯èƒ½ä¼šæ›´æ”¹å·¥å…·å
      
      logger.info(`ğŸ” === Parameter Conversion Results ===`);
      logger.info(`ğŸ” Original Tool: ${toolName}`);
      logger.info(`ğŸ” Final Tool: ${finalToolName}`);
      logger.info(`ğŸ” Original Args: ${JSON.stringify(originalArgs, null, 2)}`);
      logger.info(`ğŸ” Converted Params: ${JSON.stringify(convertedParams, null, 2)}`);
      logger.info(`ğŸ” Conversion reasoning: ${conversion.reasoning || 'No reasoning provided'}`);
      logger.info(`ğŸ” =====================================`);
      
      return { toolName: finalToolName, params: convertedParams };

    } catch (error) {
      logger.error(`âŒ Parameter conversion failed:`, error);
      return { toolName, params: originalArgs }; // å›é€€åˆ°åŸå§‹å‚æ•°
    }
  }

  /**
   * Twitterå·¥ä½œæµæœ€åæ£€æŸ¥ï¼šå¦‚æœLLMæ²¡æœ‰æ­£ç¡®å¤„ç†ï¼Œè¿›è¡Œæœ€åçš„å®‰å…¨ä¿®å¤
   */
  private async handleTwitterWorkflowFix(
    mcpName: string,
    toolName: string,
    input: any,
    state: EnhancedWorkflowState,
    userId?: string
  ): Promise<{ finalToolName: string; finalInput: any }> {
    // LLMåº”è¯¥å·²ç»å¤„ç†äº†å¤§éƒ¨åˆ†æƒ…å†µï¼Œè¿™é‡Œåªæ˜¯æœ€åçš„å®‰å…¨æ£€æŸ¥
    const normalizedMcpName = this.normalizeMCPName(mcpName);
    if (normalizedMcpName !== 'x-mcp') {
      return { finalToolName: toolName, finalInput: input };
    }

    logger.info(`ğŸ” Twitterå·¥ä½œæµæœ€åæ£€æŸ¥: ${toolName}`);

    // å¦‚æœæ˜¯publish_draftä½†ä»ç„¶æœ‰å ä½ç¬¦draft_idï¼Œä½œä¸ºæœ€åçš„å®‰å…¨ç½‘
    if (toolName === 'publish_draft' && input.draft_id && this.isPlaceholderDraftId(input.draft_id)) {
      logger.warn(`âš ï¸ Twitterå·¥ä½œæµå®‰å…¨ä¿®å¤: LLMæœªèƒ½å¤„ç†å ä½ç¬¦draft_id="${input.draft_id}"ï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ°create_draft_thread`);
      
      const content = this.extractContentFromState(state);
      if (content) {
        const optimizedContent = await this.optimizeContentForTwitterThread(content, userId);
        return {
          finalToolName: 'create_draft_thread',
          finalInput: { content: optimizedContent }
        };
      }
    }

    return { finalToolName: toolName, finalInput: input };
  }

  /**
   * æ£€æµ‹æ˜¯å¦ä¸ºå ä½ç¬¦draft_id
   */
  private isPlaceholderDraftId(draftId: string): boolean {
    if (!draftId) return false;
    
    // å¸¸è§çš„å ä½ç¬¦æ¨¡å¼
    const placeholderPatterns = [
      /^12345$/,                          // å¸¸è§å ä½ç¬¦ "12345"
      /^draft_?id$/i,                     // "draft_id" æˆ– "draftid"
      /^placeholder/i,                    // "placeholder..."
      /^example/i,                        // "example..."
      /^sample/i,                         // "sample..."
      /^test/i,                           // "test..."
      /^dummy/i,                          // "dummy..."
      /^\d{1,5}$/,                        // ç®€å•æ•°å­— 1-99999
      /^[a-z]+_\d{1,3}$/i,               // "draft_1", "test_123" ç­‰
      /^[a-z]+\d{1,3}$/i,                // "draft1", "test123" ç­‰
      /^your_draft_id$/i,                 // "your_draft_id"
      /^actual_draft_id$/i,               // "actual_draft_id"
      /^\[.*\]$/,                         // "[Insert Draft ID]" ç­‰
      /^<.*>$/,                           // "<DRAFT_ID>" ç­‰
      /^\{.*\}$/                          // "{draft_id}" ç­‰
    ];
    
    for (const pattern of placeholderPatterns) {
      if (pattern.test(draftId.trim())) {
        return true;
      }
    }
    
    // æ£€æŸ¥æ˜¯å¦åŒ…å«å ä½ç¬¦å…³é”®è¯
    const placeholderKeywords = [
      'insert', 'replace', 'actual', 'real', 'valid', 'specific',
      'here', 'id', 'placeholder', 'example', 'sample', 'template'
    ];
    
    const lowerDraftId = draftId.toLowerCase();
    for (const keyword of placeholderKeywords) {
      if (lowerDraftId.includes(keyword)) {
        return true;
      }
    }
    
    return false;
  }

  /**
   * ä½¿ç”¨LLMä¼˜åŒ–å†…å®¹ä¸ºTwitter threadæ ¼å¼
   */
  private async optimizeContentForTwitterThread(content: string, userId?: string): Promise<string> {
    if (!content || content.trim().length === 0) {
      return content;
    }

    try {
      const optimizationPrompt = `You are a social media content optimization expert. Your task is to transform the provided content into an engaging Twitter thread format.

**Original Content:**
${content}

**Optimization Rules:**
1. **Character Limit**: Each tweet in the thread must be under 280 characters
2. **Thread Structure**: Start with "ğŸ§µ Thread" or similar indicator
3. **Engagement**: Use emojis, hashtags, and engaging language
4. **Clarity**: Break complex information into digestible chunks
5. **URLs**: Preserve any URLs from the original content
6. **Numbering**: Use 1/n, 2/n format if content is long enough for multiple tweets

**Output Requirements:**
- If content is under 250 characters: Return as single optimized tweet
- If content is longer: Structure as a thread with clear breaks
- Maintain the key information and insights from original content
- Make it engaging and Twitter-appropriate

Transform the content now:`;

      const response = await this.llm.invoke([new SystemMessage(optimizationPrompt)]);
      const optimizedContent = response.content.toString().trim();
      
      logger.info(`ğŸ“ Twitterå†…å®¹ä¼˜åŒ–: åŸå§‹é•¿åº¦=${content.length}, ä¼˜åŒ–åé•¿åº¦=${optimizedContent.length}`);
      
      return optimizedContent;
      
    } catch (error) {
      logger.error(`âŒ Twitterå†…å®¹ä¼˜åŒ–å¤±è´¥:`, error);
      // å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹çš„æˆªæ–­ç‰ˆæœ¬
      return content.length > 250 ? content.substring(0, 247) + '...' : content;
    }
  }

  /**
   * ä»æ‰§è¡ŒçŠ¶æ€ä¸­æå–å¯å‘å¸ƒçš„å†…å®¹
   */
  private extractContentFromState(state: EnhancedWorkflowState): string | null {
    // æ£€æŸ¥dataStoreä¸­çš„å†…å®¹
    if (state.dataStore.lastResult) {
      const lastResult = state.dataStore.lastResult;
      
      // å°è¯•æå–æ–‡æœ¬å†…å®¹
      if (typeof lastResult === 'string') {
        return lastResult;
      }
      
      if (lastResult && typeof lastResult === 'object') {
        // æ£€æŸ¥å¸¸è§çš„å†…å®¹å­—æ®µ
        if (lastResult.content) return lastResult.content;
        if (lastResult.text) return lastResult.text;
        if (lastResult.summary) return lastResult.summary;
        if (lastResult.result) return lastResult.result;
        
        // æ£€æŸ¥MCPæ ¼å¼çš„contentæ•°ç»„
        if (lastResult.content && Array.isArray(lastResult.content)) {
          for (const item of lastResult.content) {
            if (item && item.text) {
              return item.text;
            }
          }
        }
        
        // å¦‚æœæ˜¯å¤æ‚å¯¹è±¡ï¼Œå°†å…¶è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬
        return JSON.stringify(lastResult, null, 2);
      }
    }
    
    // æ£€æŸ¥æ‰§è¡Œå†å²ä¸­çš„æœ€æ–°æ•°æ®
    if (state.executionHistory.length > 0) {
      const lastStep = state.executionHistory[state.executionHistory.length - 1];
      if (lastStep.success && lastStep.result) {
        const result = lastStep.result;
        
        if (typeof result === 'string') {
          return result;
        }
        
        if (result && typeof result === 'object') {
          if (result.content) return result.content;
          if (result.text) return result.text;
          if (result.summary) return result.summary;
          
          // è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬
          return JSON.stringify(result, null, 2);
        }
      }
    }
    
    return null;
  }

  /**
   * x-mcpè‡ªåŠ¨å‘å¸ƒå¤„ç†ï¼šå½“create_draft_tweetæˆ–create_draft_threadæˆåŠŸåè‡ªåŠ¨å‘å¸ƒ
   * ğŸ”§ ä¸AgentIntelligentEngineä¿æŒå®Œå…¨ä¸€è‡´
   */
  private async handleXMcpAutoPublish(
    mcpName: string, 
    toolName: string, 
    result: any, 
    userId?: string
  ): Promise<any> {
    // ğŸ”§ æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
    const normalizedMcpName = this.normalizeMCPName(mcpName);
    logger.info(`ğŸ” EnhancedEngine X-MCP Auto-publish Check: mcpName="${mcpName}", normalizedMcpName="${normalizedMcpName}", toolName="${toolName}"`);
    
    // åªå¤„ç†x-mcpçš„è‰ç¨¿åˆ›å»ºæ“ä½œ
    if (normalizedMcpName !== 'x-mcp') {
      logger.info(`âŒ EnhancedEngine X-MCP Auto-publish: Normalized MCP name "${normalizedMcpName}" is not "x-mcp", skipping auto-publish`);
      return result;
    }

    // æ£€æŸ¥æ˜¯å¦æ˜¯è‰ç¨¿åˆ›å»ºæ“ä½œ
    if (!toolName.includes('create_draft')) {
      logger.info(`âŒ EnhancedEngine X-MCP Auto-publish: Tool name "${toolName}" does not include "create_draft", skipping auto-publish`);
      return result;
    }

    try {
      logger.info(`ğŸ”„ X-MCP Auto-publish: Detected ${toolName} completion, attempting auto-publish...`);

      // æå–draft_id - ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´çš„é€»è¾‘
      let draftId = null;
      if (result && typeof result === 'object') {
        // å°è¯•ä»ä¸åŒçš„ç»“æœæ ¼å¼ä¸­æå–draft_id
        if (result.draft_id) {
          draftId = result.draft_id;
        } else if (result.content && Array.isArray(result.content)) {
          // MCPæ ‡å‡†æ ¼å¼
          for (const content of result.content) {
            if (content.text) {
              try {
                const parsed = JSON.parse(content.text);
                if (parsed.draft_id) {
                  draftId = parsed.draft_id;
                  break;
                } else if (Array.isArray(parsed)) {
                  // ğŸ”§ å¤„ç†è§£ææˆåŠŸä½†æ˜¯æ•°ç»„ç»“æ„çš„æƒ…å†µ
                  for (const nestedItem of parsed) {
                    if (nestedItem && nestedItem.text) {
                      const innerText = nestedItem.text;
                      const innerMatch = this.extractDraftIdFromText(innerText);
                      if (innerMatch) {
                        draftId = innerMatch;
                        logger.info(`ğŸ“ EnhancedEngine X-MCP Auto-publish: Extracted draft_id "${draftId}" from nested JSON structure`);
                        break;
                      }
                    }
                  }
                  if (draftId) break;
                }
              } catch {
                // ğŸ”§ ä¿®å¤ï¼šå¤„ç†åµŒå¥—JSONç»“æ„å’Œæ–‡æœ¬æå–
                let text = content.text;
                
                // ğŸ”§ å¤„ç†åŒé‡åµŒå¥—çš„JSONæƒ…å†µï¼štextå­—æ®µæœ¬èº«æ˜¯JSONå­—ç¬¦ä¸²
                try {
                  // å°è¯•è§£ætextä½œä¸ºJSONæ•°ç»„
                  const nestedArray = JSON.parse(text);
                  if (Array.isArray(nestedArray)) {
                    // éå†åµŒå¥—æ•°ç»„ï¼Œå¯»æ‰¾åŒ…å«draftä¿¡æ¯çš„æ–‡æœ¬
                    for (const nestedItem of nestedArray) {
                      if (nestedItem && nestedItem.text) {
                        const innerText = nestedItem.text;
                        // å…ˆå°è¯•ä»å†…å±‚æ–‡æœ¬æå–draft_id
                        const innerMatch = this.extractDraftIdFromText(innerText);
                        if (innerMatch) {
                          draftId = innerMatch;
                          logger.info(`ğŸ“ EnhancedEngine X-MCP Auto-publish: Extracted draft_id "${draftId}" from nested JSON structure`);
                          break;
                        }
                      }
                    }
                  }
                } catch {
                  // å¦‚æœä¸æ˜¯JSONï¼Œç»§ç»­ç”¨åŸæ–‡æœ¬è¿›è¡Œæ¨¡å¼åŒ¹é…
                }
                
                // å¦‚æœåµŒå¥—è§£ææ²¡æœ‰æ‰¾åˆ°ï¼Œç”¨åŸæ–‡æœ¬è¿›è¡Œæ¨¡å¼åŒ¹é…
                if (!draftId) {
                  draftId = this.extractDraftIdFromText(text);
                  if (draftId) {
                    logger.info(`ğŸ“ EnhancedEngine X-MCP Auto-publish: Extracted draft_id "${draftId}" from text pattern matching`);
                  }
                }
                
                if (draftId) break;
              }
            }
          }
        }
      }
      
      // ğŸ”§ ä¿®å¤ï¼šå¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„result
      if (!draftId && typeof result === 'string') {
        // ä»å­—ç¬¦ä¸²ç»“æœä¸­æå–
        try {
          const parsed = JSON.parse(result);
          if (parsed.draft_id) {
            draftId = parsed.draft_id;
          }
        } catch {
          // ğŸ”§ ä¿®å¤ï¼šå¤„ç†åµŒå¥—JSONå’Œå­—ç¬¦ä¸²æ–‡æœ¬ä¸­æå–draft ID
          draftId = this.extractDraftIdFromText(result);
          if (draftId) {
            logger.info(`ğŸ“ EnhancedEngine X-MCP Auto-publish: Extracted draft_id "${draftId}" from string pattern matching`);
          }
        }
      }

      if (!draftId) {
        logger.warn(`âš ï¸ X-MCP Auto-publish: Could not extract draft_id from result: ${JSON.stringify(result)}`);
        return result;
      }

      logger.info(`ğŸ“ X-MCP Auto-publish: Extracted draft_id: ${draftId}`);

      // è°ƒç”¨publish_draft
      logger.info(`ğŸš€ X-MCP Auto-publish: Publishing draft ${draftId}...`);
      
      const publishInput = { draft_id: draftId };
      logger.info(`ğŸ“ EnhancedEngine X-MCP Auto-publish INPUT: ${JSON.stringify(publishInput, null, 2)}`);
      
      const publishResult = await this.mcpToolAdapter.callTool(
        normalizedMcpName,
        'publish_draft',
        publishInput,
        userId
      );
      
      logger.info(`ğŸ“¤ EnhancedEngine X-MCP Auto-publish OUTPUT: ${JSON.stringify(publishResult, null, 2)}`);

      logger.info(`âœ… X-MCP Auto-publish: Successfully published draft ${draftId}`);

      // è¿”å›åˆå¹¶çš„ç»“æœ - ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´
      return {
        draft_creation: result,
        auto_publish: publishResult,
        combined_result: `Draft created and published successfully. Draft ID: ${draftId}`,
        draft_id: draftId,
        published: true
      };

    } catch (error) {
      logger.error(`âŒ X-MCP Auto-publish failed:`, error);
      
      // å³ä½¿å‘å¸ƒå¤±è´¥ï¼Œä¹Ÿè¿”å›åŸå§‹çš„è‰ç¨¿åˆ›å»ºç»“æœ - ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´
      return {
        draft_creation: result,
        auto_publish_error: error instanceof Error ? error.message : String(error),
        combined_result: `Draft created successfully but auto-publish failed. You may need to publish manually.`,
        published: false
      };
    }
  }

  /**
   * ä»æ–‡æœ¬ä¸­æå–draft_idçš„è¾…åŠ©æ–¹æ³• - ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´
   */
  private extractDraftIdFromText(text: string): string | null {
    const patterns = [
      /draft[_-]?id["\s:]*([^"\s,}]+)/i,                    // draft_id: "xxx" 
      /with\s+id\s+([a-zA-Z0-9_.-]+\.json)/i,               // "with ID thread_draft_xxx.json"
      /created\s+with\s+id\s+([a-zA-Z0-9_.-]+\.json)/i,     // "created with ID xxx.json"
      /id[:\s]+([a-zA-Z0-9_.-]+\.json)/i,                   // "ID: xxx.json" æˆ– "ID xxx.json"
      /([a-zA-Z0-9_.-]*draft[a-zA-Z0-9_.-]*\.json)/i        // ä»»ä½•åŒ…å«draftçš„.jsonæ–‡ä»¶
    ];
    
    for (const pattern of patterns) {
      const match = text.match(pattern);
      if (match) {
        return match[1];
      }
    }
    
    return null;
  }

  /**
   * æ ‡å‡†åŒ–MCPåç§° - ä¸Agentå¼•æ“å®Œå…¨ä¸€è‡´
   */
  private normalizeMCPName(mcpName: string): string {
    const nameMapping: Record<string, string> = {
      'twitter': 'twitter-client-mcp',
      'github': 'github-mcp',
      'coinmarketcap': 'coinmarketcap-mcp',
      'crypto': 'coinmarketcap-mcp',
      'web': 'brave-search-mcp',
      'search': 'brave-search-mcp',
      'x-mcp': 'x-mcp'  // ğŸ”§ æ·»åŠ x-mcpçš„æ˜ å°„
    };

    return nameMapping[mcpName.toLowerCase()] || mcpName;
  }

  /**
   * ğŸ”§ æ–°å¢ï¼šé¢„å¤„ç†å‚æ•°åï¼ˆcamelCase åˆ° snake_caseï¼‰
   */
  private preprocessParameterNames(originalArgs: any, inputSchema: any): any {
    if (!originalArgs || typeof originalArgs !== 'object') {
      return originalArgs;
    }

    const schemaProperties = inputSchema.properties || {};
    const expectedParamNames = Object.keys(schemaProperties);
    
    logger.info(`ğŸ”§ Preprocessing parameters, expected: [${expectedParamNames.join(', ')}]`);

    const processedArgs: any = {};
    
    for (const [key, value] of Object.entries(originalArgs)) {
      let mappedKey = key;
      
      // æ£€æŸ¥æ˜¯å¦éœ€è¦ camelCase -> snake_case è½¬æ¢
      if (!expectedParamNames.includes(key)) {
        const snakeCaseKey = this.camelToSnakeCase(key);
        if (expectedParamNames.includes(snakeCaseKey)) {
          mappedKey = snakeCaseKey;
          logger.info(`ğŸ”§ Parameter name mapped: ${key} -> ${mappedKey}`);
        }
      }
      
      processedArgs[mappedKey] = value;
    }

    return processedArgs;
  }

  /**
   * ğŸ”§ æ–°å¢ï¼šcamelCase è½¬ snake_case
   */
  private camelToSnakeCase(str: string): string {
    return str.replace(/([a-z])([A-Z])/g, '$1_$2').toLowerCase();
  }


}

/**
 * å¢å¼ºçš„æ™ºèƒ½TaskæœåŠ¡ - åŸºäºå·¥ä½œæµæ‰§è¡Œ
 */
export class EnhancedIntelligentTaskService {
  private engine: EnhancedIntelligentTaskEngine;
  private taskService: any;

  constructor() {
    this.engine = new EnhancedIntelligentTaskEngine();
    this.taskService = getTaskService();
  }

  /**
   * æ‰§è¡Œå¢å¼ºçš„æ™ºèƒ½Task - åŸºäºå·²æ„å»ºçš„å·¥ä½œæµ
   */
  async executeTaskEnhanced(
    taskId: string,
    stream: (data: any) => void,
    skipAnalysisCheck: boolean = false
  ): Promise<boolean> {
    try {
      logger.info(`âš¡ Starting enhanced workflow-based task execution [Task: ${taskId}]`);

      // è·å–ä»»åŠ¡ä¿¡æ¯
      const task = await this.taskService.getTaskById(taskId);
      if (!task) {
        // ğŸ”§ å‘é€é”™è¯¯äº‹ä»¶
        stream({ 
          event: 'error', 
          data: { 
            message: 'Task not found'
          }
        });
        return false;
      }

      // æ£€æŸ¥æ˜¯å¦å·²æœ‰å·¥ä½œæµ
      const mcpWorkflow = typeof task.mcpWorkflow === 'string' 
        ? JSON.parse(task.mcpWorkflow) 
        : task.mcpWorkflow;

      if (!skipAnalysisCheck && (!mcpWorkflow || !mcpWorkflow.workflow || mcpWorkflow.workflow.length === 0)) {
        // ğŸ”§ å‘é€é”™è¯¯äº‹ä»¶
        stream({ 
          event: 'error', 
          data: { 
            message: 'No workflow found. Please analyze the task first.',
            details: 'Call /api/task/:id/analyze to generate a workflow before execution.'
          }
        });
        return false;
      }

      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      await taskExecutorDao.updateTaskStatus(taskId, 'in_progress');
      // ğŸ”§ å‘é€çŠ¶æ€æ›´æ–°äº‹ä»¶
      stream({ 
        event: 'status_update', 
        data: { 
          status: 'in_progress'
        }
      });

      // ä½¿ç”¨å¢å¼ºå¼•æ“æ‰§è¡Œå·¥ä½œæµ
      const executionGenerator = this.engine.executeWorkflowEnhanced(taskId, mcpWorkflow);

      let finalSuccess = false;

      for await (const result of executionGenerator) {
        // ğŸ”§ ç›´æ¥æµå¼ä¼ è¾“åŸå§‹äº‹ä»¶ï¼Œä¸åŒ…è£…
        stream(result);
        
        // è®°å½•æœ€ç»ˆæ‰§è¡Œç»“æœ
        if (result.event === 'final_result') {
          finalSuccess = result.data.success;
        }
      }

      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      await taskExecutorDao.updateTaskStatus(
        taskId, 
        finalSuccess ? 'completed' : 'failed'
      );

      // ğŸ”§ å‘é€æ‰§è¡Œå®Œæˆäº‹ä»¶
      stream({
        event: 'task_execution_complete',
        data: {
          success: finalSuccess,
          message: finalSuccess ? 
            'Task execution completed successfully' : 
            'Task execution failed'
        }
      });

      logger.info(`âœ… Enhanced workflow execution completed [Task: ${taskId}, Success: ${finalSuccess}]`);
      return finalSuccess;

    } catch (error) {
      logger.error(`âŒ Enhanced workflow execution failed:`, error);
      
      // ğŸ”§ å‘é€é”™è¯¯äº‹ä»¶
      stream({
        event: 'error',
        data: {
          message: 'Enhanced workflow execution failed',
          details: error instanceof Error ? error.message : String(error)
        }
      });

      await taskExecutorDao.updateTaskStatus(taskId, 'failed');
      return false;
    }
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
export const enhancedIntelligentTaskService = new EnhancedIntelligentTaskService(); 